import torch
import torch.nn as nn

class E8TrialitySparseRegularizer(nn.Module):
    """
    E8-inspired triality regularizer for sparse-data robustness.
    Cycles three representations to enforce invariance even under heavy masking.
    """
    def __init__(self, d_model: int = 240, heads: int = 3, dropout: float = 0.1):
        super().__init__()
        assert d_model % heads == 0, "d_model must be divisible by heads"
        self.heads = heads
        self.d_head = d_model // heads
        
        # Projection to split into triality heads + fusion weights
        self.proj_split = nn.Linear(d_model, d_model)
        self.proj_fuse  = nn.Linear(d_model, d_model)
        
        # Learned weighting for cycled representations (softmax normalized)
        self.weight = nn.Parameter(torch.ones(heads))
        
        self.dropout = nn.Dropout(dropout)
        self.norm = nn.LayerNorm(d_model)

    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:
        """
        x:    [batch, seq, d_model] - usually post-attention output
        mask: [batch, seq] or [batch, seq, seq] - attention mask (1=attend, 0=masked)
        """
        batch, seq, d_model = x.shape
        
        # Project and split into triality heads
        split = self.proj_split(x)                     # [B, S, D]
        split = split.view(batch, seq, self.heads, self.d_head)  # [B, S, H, d_head]
        split = split.permute(2, 0, 1, 3)              # [H, B, S, d_head]
        
        # Triality cycle: roll the heads (0→1→2→0)
        cycled = torch.roll(split, shifts=1, dims=0)   # cycle along head dimension
        
        # Learned weighting + softmax normalization
        weights = torch.softmax(self.weight, dim=0)     # [H]
        weights = weights.view(self.heads, 1, 1, 1)     # broadcastable
        
        # Weighted fusion of cycled representations
        fused = (cycled * weights).sum(dim=0)          # [B, S, d_head]
        fused = fused.view(batch, seq, d_model)        # [B, S, D]
        
        # Project back to original space + residual
        out = self.proj_fuse(fused)
        out = self.dropout(out)
        
        # Optional: mask-aware normalization (ignore padded/sparse positions)
        if mask is not None:
            mask = mask.unsqueeze(-1).expand(-1, -1, d_model)
            out = out.masked_fill(~mask.bool(), 0.0)
        
        out = self.norm(out + x)   # residual connection
        
        return out