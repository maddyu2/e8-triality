{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51ccb79f-4a01-43cf-aff9-6db3df977fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "function ClickConnect(){\n",
       "  console.log(\"Keeping Colab alive\"); \n",
       "  document.querySelector(\"colab-toolbar-button#connect\").click() \n",
       "}\n",
       "setInterval(ClickConnect,60000)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keep-alive activated — tab clicks every 60s\n"
     ]
    }
   ],
   "source": [
    "# Keep-alive script (prevents tab idle/disconnect — click every 60s)\n",
    "from IPython.display import display, Javascript\n",
    "display(Javascript('''\n",
    "function ClickConnect(){\n",
    "  console.log(\"Keeping Colab alive\"); \n",
    "  document.querySelector(\"colab-toolbar-button#connect\").click() \n",
    "}\n",
    "setInterval(ClickConnect,60000)\n",
    "'''))\n",
    "print(\"Keep-alive activated — tab clicks every 60s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bee0567-7e05-49ab-bf07-2a69e5d236ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.4.1+cu124)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.19.1+cu124)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.8)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.3.5)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (5.0.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.2.65 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.2.65)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.0.44 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.0.44)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.119 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.119)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.0.99 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.0.99)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.0.142 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.0.142)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.99)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.99)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pyparsing>=3 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.3.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2026.1.15)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.27.2)\n",
      "Requirement already satisfied: shellingham in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer-slim->transformers) (8.3.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.6.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.5)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.10)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.14.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Using device: cuda\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.54 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.45 GiB is free. Process 565 has 1.53 GiB memory in use. Process 1825 has 1.53 GiB memory in use. Process 6071 has 17.26 GiB memory in use. Process 7871 has 10.62 GiB memory in use. Process 8777 has 1.53 GiB memory in use. Process 8996 has 912.00 MiB memory in use. Process 8744 has 1.53 GiB memory in use. Process 9642 has 1.53 GiB memory in use. Process 9860 has 37.11 GiB memory in use. Process 10046 has 1.53 GiB memory in use. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 1.93 GiB is allocated by PyTorch, and 28.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 149\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16) \u001b[38;5;28;01mif\u001b[39;00m use_amp \u001b[38;5;28;01melse\u001b[39;00m nullcontext():\n\u001b[1;32m    148\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model(real_data, epoch)\n\u001b[0;32m--> 149\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m accum_steps\n\u001b[1;32m    151\u001b[0m     logits_abl \u001b[38;5;241m=\u001b[39m model_ablation(real_data, epoch)\n\u001b[1;32m    152\u001b[0m     loss_abl \u001b[38;5;241m=\u001b[39m loss_fn(logits_abl\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, tokenizer\u001b[38;5;241m.\u001b[39mvocab_size), target\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m/\u001b[39m accum_steps\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:1188\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:3104\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3103\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.54 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.45 GiB is free. Process 565 has 1.53 GiB memory in use. Process 1825 has 1.53 GiB memory in use. Process 6071 has 17.26 GiB memory in use. Process 7871 has 10.62 GiB memory in use. Process 8777 has 1.53 GiB memory in use. Process 8996 has 912.00 MiB memory in use. Process 8744 has 1.53 GiB memory in use. Process 9642 has 1.53 GiB memory in use. Process 9860 has 37.11 GiB memory in use. Process 10046 has 1.53 GiB memory in use. Including non-PyTorch memory, this process has 2.62 GiB memory in use. Of the allocated memory 1.93 GiB is allocated by PyTorch, and 28.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# First cell: Set env + install (run once)\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # fragmentation fix\n",
    "\n",
    "!pip install torch torchvision matplotlib numpy transformers\n",
    "\n",
    "# Second cell: The sim code (fixed — no backward error)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.amp\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from contextlib import nullcontext\n",
    "import math\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# CONFIG – optimized for no OOM + H100 speed (fast epochs)\n",
    "triality = 3\n",
    "dim = 192  # reduced\n",
    "latent_dim = 8\n",
    "seq_len = 512  # reduced\n",
    "batch_size = 16  # reduced\n",
    "accum_steps = 4  # simulate larger batch\n",
    "epochs = 20000\n",
    "lr = 5e-5\n",
    "use_amp = True\n",
    "use_checkpoint = True\n",
    "\n",
    "# Synthetic code proxy (real-like Python tokens + noise/masking)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "codes = []\n",
    "for b in range(batch_size):\n",
    "    base = torch.sin(torch.linspace(0, 10*math.pi, seq_len, device=device)) * 100 + 200\n",
    "    base = base % tokenizer.vocab_size\n",
    "    code = base.long() + torch.randint(-50, 50, (seq_len,), device=device)\n",
    "    code = torch.clamp(code, 0, tokenizer.vocab_size - 1)\n",
    "    codes.append(code)\n",
    "\n",
    "codes = torch.stack(codes).to(device)\n",
    "\n",
    "embed = nn.Embedding(tokenizer.vocab_size, dim).to(device)\n",
    "clean_data = embed(codes)\n",
    "\n",
    "missing_rate = torch.linspace(0.4, 0.7, batch_size, device=device).view(batch_size, 1, 1)\n",
    "mask = torch.rand_like(clean_data) < missing_rate\n",
    "real_data = clean_data.clone()\n",
    "real_data[mask] = 0\n",
    "\n",
    "target = codes\n",
    "\n",
    "# E8 roots – precompute\n",
    "def get_e8_roots():\n",
    "    roots = []\n",
    "    for i in range(8):\n",
    "        for j in range(i+1, 8):\n",
    "            for signs in [(1,1), (1,-1), (-1,1), (-1,-1)]:\n",
    "                v = torch.zeros(8)\n",
    "                v[i] = signs[0]; v[j] = signs[1]\n",
    "                roots.append(v); roots.append(-v)\n",
    "    for signs in range(1 << 8):\n",
    "        v = torch.tensor([(1 if (signs & (1<<k)) else -1) for k in range(8)], dtype=torch.float32) * 0.5\n",
    "        if bin(signs).count('1') % 2 == 0:\n",
    "            roots.append(v); roots.append(-v)\n",
    "    roots = torch.stack(roots[:240])\n",
    "    return roots / roots.norm(dim=-1, keepdim=True)\n",
    "\n",
    "e8_roots = get_e8_roots().to(device)\n",
    "\n",
    "# Triality Cycle Block (detached pump scalar)\n",
    "class CodeCycleBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(latent_dim, dim // triality, bias=False)\n",
    "        self.register_buffer('roots', e8_roots)\n",
    "\n",
    "    def forward(self, x, step):\n",
    "        pos_emb = self.roots[torch.arange(x.shape[1], device=device) % 240]\n",
    "        low_dim = self.proj(pos_emb)\n",
    "        emb = low_dim.repeat(1, triality)\n",
    "        with torch.no_grad():\n",
    "            pump_scalar = 0.8 * math.sin(step * 0.006 * 2 * math.pi)\n",
    "        pump = torch.full((1, x.shape[1], 1), pump_scalar, device=device)\n",
    "        emb_broadcast = emb.unsqueeze(0)\n",
    "        x_rot1 = x * (emb_broadcast.cos() + pump)\n",
    "        x_rot2 = torch.roll(x_rot1, shifts=1, dims=1) * emb_broadcast.sin()\n",
    "        x_rot3 = torch.roll(x_rot2, shifts=1, dims=1) * emb_broadcast.cos()\n",
    "        fused = (x_rot1 + x_rot2 + x_rot3) / triality\n",
    "        return fused\n",
    "\n",
    "# Dummy cycle for ablation\n",
    "class DummyCycle(nn.Module):\n",
    "    def forward(self, x, step=None):\n",
    "        return x\n",
    "\n",
    "# Model with ablation support (reduced depth)\n",
    "class E8CodeFusion(nn.Module):\n",
    "    def __init__(self, depth=16, use_triality=True):  # reduced\n",
    "        super().__init__()\n",
    "        self.use_triality = use_triality\n",
    "        self.cycle = CodeCycleBlock() if use_triality else DummyCycle()\n",
    "        num_heads = triality if use_triality else 8\n",
    "        self.layers = nn.ModuleList([nn.MultiheadAttention(dim, num_heads, batch_first=True) for _ in range(depth)])\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.head = nn.Linear(dim, tokenizer.vocab_size)\n",
    "\n",
    "    def forward(self, x, step):\n",
    "        x = self.cycle(x, step)\n",
    "        for layer in self.layers:\n",
    "            if use_checkpoint:\n",
    "                attn, _ = checkpoint(layer, x, x, x, use_reentrant=False)\n",
    "            else:\n",
    "                attn, _ = layer(x, x, x)\n",
    "            x = x + self.norm(attn)\n",
    "        return self.head(x)\n",
    "\n",
    "# Models\n",
    "model = E8CodeFusion(use_triality=True).to(device)\n",
    "model_ablation = E8CodeFusion(use_triality=False).to(device)\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scaler = torch.amp.GradScaler('cuda') if use_amp else nullcontext()\n",
    "\n",
    "opt_ablation = torch.optim.AdamW(model_ablation.parameters(), lr=lr)\n",
    "scaler_ablation = torch.amp.GradScaler('cuda') if use_amp else nullcontext()\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "loss_hist = []\n",
    "loss_abl_hist = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    opt_ablation.zero_grad(set_to_none=True)\n",
    "\n",
    "    accum_loss = 0\n",
    "    accum_loss_abl = 0\n",
    "\n",
    "    for accum_step in range(accum_steps):\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.float16) if use_amp else nullcontext():\n",
    "            logits = model(real_data, epoch)\n",
    "            loss = loss_fn(logits.view(-1, tokenizer.vocab_size), target.view(-1)) / accum_steps\n",
    "\n",
    "            logits_abl = model_ablation(real_data, epoch)\n",
    "            loss_abl = loss_fn(logits_abl.view(-1, tokenizer.vocab_size), target.view(-1)) / accum_steps\n",
    "\n",
    "        scaler.scale(loss).backward() if use_amp else loss.backward()\n",
    "        scaler_ablation.scale(loss_abl).backward() if use_amp else loss_abl.backward()\n",
    "\n",
    "        accum_loss += loss.item() * accum_steps\n",
    "        accum_loss_abl += loss_abl.item() * accum_steps\n",
    "\n",
    "    scaler.unscale_(opt) if use_amp else None\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1e6)\n",
    "    scaler.step(opt) if use_amp else opt.step()\n",
    "    scaler.update() if use_amp else None\n",
    "\n",
    "    scaler_ablation.unscale_(opt_ablation) if use_amp else None\n",
    "    torch.nn.utils.clip_grad_norm_(model_ablation.parameters(), 1e6)\n",
    "    scaler_ablation.step(opt_ablation) if use_amp else opt_ablation.step()\n",
    "    scaler_ablation.update() if use_amp else None\n",
    "\n",
    "    loss_hist.append(accum_loss)\n",
    "    loss_abl_hist.append(accum_loss_abl)\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch {epoch} | Triality Loss {accum_loss:.6f} | Ablation Loss {accum_loss_abl:.6f}\")\n",
    "\n",
    "# Final Sigma Test\n",
    "triality_mean = np.mean(loss_hist)\n",
    "abl_mean = np.mean(loss_abl_hist)\n",
    "std = np.std(loss_hist + loss_abl_hist)\n",
    "sigma = (abl_mean - triality_mean) / std if std > 0 else 0\n",
    "\n",
    "print(f\"Final Sigma (Triality vs Ablation): {sigma:.2f} (higher = triality advantage)\")\n",
    "\n",
    "print(\"Sim complete — epochs + sigma test done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3778d141-62e8-4ec9-ab72-eee4c297760e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
