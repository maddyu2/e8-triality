{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d01fcb3-db99-42c6-90d6-7503a66c9a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.4.1+cu124)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.19.1+cu124)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.8)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.3.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.2.65 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.2.65)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.0.44 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.0.44)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.119 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.119)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.0.99 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.0.99)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.0.142 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.0.142)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.99)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.99)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pyparsing>=3 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Using device: cuda\n",
      "\n",
      "=== Training on Task 0 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0 Epoch 0 | Loss 0.172003\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 156\u001b[0m\n\u001b[1;32m    153\u001b[0m     outputs_abl \u001b[38;5;241m=\u001b[39m model_ablation(images, task_id \u001b[38;5;241m*\u001b[39m epochs_per_task \u001b[38;5;241m+\u001b[39m epoch)\n\u001b[1;32m    154\u001b[0m     loss_abl \u001b[38;5;241m=\u001b[39m loss_fn(outputs_abl, labels)\n\u001b[0;32m--> 156\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m use_amp \u001b[38;5;28;01melse\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    157\u001b[0m scaler\u001b[38;5;241m.\u001b[39munscale_(opt) \u001b[38;5;28;01mif\u001b[39;00m use_amp \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    158\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1e6\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# First cell: Install dependencies (run once — fast on RunPod H100)\n",
    "!pip install torch torchvision matplotlib numpy\n",
    "\n",
    "# Second cell: The sim code (fixed — no backward error)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.amp\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from contextlib import nullcontext\n",
    "import math\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# CONFIG – optimized for H100 speed (fast epochs)\n",
    "triality = 3\n",
    "dim = 384\n",
    "latent_dim = 8\n",
    "num_tasks = 10\n",
    "epochs_per_task = 5000\n",
    "lr = 5e-5\n",
    "use_amp = True\n",
    "use_checkpoint = True\n",
    "\n",
    "# Permuted MNIST continual benchmark (real handwritten digits)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "train_dataset = MNIST(root=\"./\", train=True, download=True, transform=transform)\n",
    "test_dataset = MNIST(root=\"./\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Create permuted versions for tasks\n",
    "tasks = []\n",
    "for task_id in range(num_tasks):\n",
    "    perm = torch.randperm(28*28)\n",
    "    def permute(x):\n",
    "        return x.view(-1, 28*28)[:, perm].view(-1, 1, 28, 28)\n",
    "    tasks.append(permute)\n",
    "\n",
    "# E8 roots – precompute\n",
    "def get_e8_roots():\n",
    "    roots = []\n",
    "    for i in range(8):\n",
    "        for j in range(i+1, 8):\n",
    "            for signs in [(1,1), (1,-1), (-1,1), (-1,-1)]:\n",
    "                v = torch.zeros(8)\n",
    "                v[i] = signs[0]; v[j] = signs[1]\n",
    "                roots.append(v); roots.append(-v)\n",
    "    for signs in range(1 << 8):\n",
    "        v = torch.tensor([(1 if (signs & (1<<k)) else -1) for k in range(8)], dtype=torch.float32) * 0.5\n",
    "        if bin(signs).count('1') % 2 == 0:\n",
    "            roots.append(v); roots.append(-v)\n",
    "    roots = torch.stack(roots[:240])\n",
    "    return roots / roots.norm(dim=-1, keepdim=True)\n",
    "\n",
    "e8_roots = get_e8_roots().to(device)\n",
    "\n",
    "# Triality Cycle Block (detached step + detached pump scalar)\n",
    "class ContinualCycleBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(latent_dim, dim // triality, bias=False)\n",
    "        self.register_buffer('roots', e8_roots)\n",
    "\n",
    "    def forward(self, x, step):\n",
    "        pos_emb = self.roots[torch.arange(x.shape[1], device=device) % 240]\n",
    "        low_dim = self.proj(pos_emb)\n",
    "        emb = low_dim.repeat(1, triality)\n",
    "        # Detached pump scalar (no graph change)\n",
    "        with torch.no_grad():\n",
    "            pump_scalar = 0.8 * math.sin(step * 0.006 * 2 * math.pi)\n",
    "        pump = torch.full((1, x.shape[1], 1), pump_scalar, device=device)  # (1, seq_len, 1)\n",
    "        emb_broadcast = emb.unsqueeze(0)  # (1, seq_len, dim)\n",
    "        x_rot1 = x * (emb_broadcast.cos() + pump)\n",
    "        x_rot2 = torch.roll(x_rot1, shifts=1, dims=1) * emb_broadcast.sin()\n",
    "        x_rot3 = torch.roll(x_rot2, shifts=1, dims=1) * emb_broadcast.cos()\n",
    "        fused = (x_rot1 + x_rot2 + x_rot3) / triality\n",
    "        return fused\n",
    "\n",
    "# Dummy cycle for ablation (ignores step)\n",
    "class DummyCycle(nn.Module):\n",
    "    def forward(self, x, step=None):\n",
    "        return x\n",
    "\n",
    "# Model with ablation support\n",
    "class E8ContinualLongUpdate(nn.Module):\n",
    "    def __init__(self, depth=32, use_triality=True):\n",
    "        super().__init__()\n",
    "        self.use_triality = use_triality\n",
    "        self.proj = nn.Linear(784, dim)  # project flattened MNIST to dim\n",
    "        self.cycle = ContinualCycleBlock() if use_triality else DummyCycle()\n",
    "        self.layers = nn.ModuleList([nn.MultiheadAttention(dim, triality if use_triality else 8, batch_first=True) for _ in range(depth)])\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.head = nn.Linear(dim, 10)  # MNIST classes\n",
    "\n",
    "    def forward(self, x, step):\n",
    "        x = x.view(x.size(0), -1)  # flatten to (batch, 784)\n",
    "        x = self.proj(x)  # project to dim\n",
    "        x = x.unsqueeze(1)  # (batch, 1, dim) for attention\n",
    "        x = self.cycle(x, step)\n",
    "        for layer in self.layers:\n",
    "            if use_checkpoint:\n",
    "                attn, _ = checkpoint(layer, x, x, x, use_reentrant=False)\n",
    "            else:\n",
    "                attn, _ = layer(x, x, x)\n",
    "            x = x + self.norm(attn)\n",
    "        return self.head(x.mean(dim=1))\n",
    "\n",
    "# Models\n",
    "model = E8ContinualLongUpdate(use_triality=True).to(device)\n",
    "\n",
    "model_ablation = E8ContinualLongUpdate(use_triality=False).to(device)\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scaler = torch.amp.GradScaler('cuda') if use_amp else nullcontext()\n",
    "\n",
    "opt_ablation = torch.optim.AdamW(model_ablation.parameters(), lr=lr)\n",
    "scaler_ablation = torch.amp.GradScaler('cuda') if use_amp else nullcontext()\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Continual training across tasks\n",
    "accuracy_hist = {i: [] for i in range(num_tasks)}\n",
    "accuracy_abl_hist = {i: [] for i in range(num_tasks)}\n",
    "\n",
    "for task_id in range(num_tasks):\n",
    "    print(f\"\\n=== Training on Task {task_id} ===\")\n",
    "    \n",
    "    permute = tasks[task_id]\n",
    "    \n",
    "    for epoch in range(epochs_per_task):\n",
    "        for images, labels in train_loader:\n",
    "            images = permute(images).to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            opt_ablation.zero_grad(set_to_none=True)\n",
    "\n",
    "            with torch.amp.autocast(device_type='cuda', dtype=torch.float16) if use_amp else nullcontext():\n",
    "                outputs = model(images, task_id * epochs_per_task + epoch)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "\n",
    "                outputs_abl = model_ablation(images, task_id * epochs_per_task + epoch)\n",
    "                loss_abl = loss_fn(outputs_abl, labels)\n",
    "\n",
    "            scaler.scale(loss).backward() if use_amp else loss.backward()\n",
    "            scaler.unscale_(opt) if use_amp else None\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1e6)\n",
    "            scaler.step(opt) if use_amp else opt.step()\n",
    "            scaler.update() if use_amp else None\n",
    "\n",
    "            scaler_ablation.scale(loss_abl).backward() if use_amp else loss_abl.backward()\n",
    "            scaler_ablation.unscale_(opt_ablation) if use_amp else None\n",
    "            torch.nn.utils.clip_grad_norm_(model_ablation.parameters(), 1e6)\n",
    "            scaler_ablation.step(opt_ablation) if use_amp else opt_ablation.step()\n",
    "            scaler_ablation.update() if use_amp else None\n",
    "\n",
    "        if epoch % 2000 == 0:\n",
    "            print(f\"Task {task_id} Epoch {epoch} | Loss {loss.item():.6f}\")\n",
    "\n",
    "    # Test retention on all previous tasks\n",
    "    with torch.no_grad():\n",
    "        for prev_task in range(task_id + 1):\n",
    "            permute_prev = tasks[prev_task]\n",
    "            acc = 0\n",
    "            acc_abl = 0\n",
    "            for images, labels in test_loader:\n",
    "                images = permute_prev(images).to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(images, task_id * epochs_per_task)\n",
    "                acc += (outputs.argmax(dim=1) == labels).float().mean().item()\n",
    "\n",
    "                outputs_abl = model_ablation(images, task_id * epochs_per_task)\n",
    "                acc_abl += (outputs_abl.argmax(dim=1) == labels).float().mean().item()\n",
    "\n",
    "            acc /= len(test_loader)\n",
    "            acc_abl /= len(test_loader)\n",
    "            accuracy_hist[prev_task].append(acc)\n",
    "            accuracy_abl_hist[prev_task].append(acc_abl)\n",
    "\n",
    "# Sigma Retention Test\n",
    "e8_retention = np.mean([accuracy_hist[i][-1] for i in range(num_tasks)])\n",
    "abl_retention = np.mean([accuracy_abl_hist[i][-1] for i in range(num_tasks)])\n",
    "ret_std = np.std([accuracy_hist[i][-1] for i in range(num_tasks)] + [accuracy_abl_hist[i][-1] for i in range(num_tasks)])\n",
    "sigma_retention = (e8_retention - abl_retention) / ret_std if ret_std > 0 else 0\n",
    "\n",
    "print(f\"Final Retention Sigma: {sigma_retention:.2f}\")\n",
    "\n",
    "print(\"Sim complete — epochs + sigma test done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a922a48b-8160-4910-ae70-3ee41550693b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
