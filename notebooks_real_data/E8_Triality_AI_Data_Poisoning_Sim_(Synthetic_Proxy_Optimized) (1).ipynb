{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svk2LXplqElC"
      },
      "outputs": [],
      "source": [
        "# First cell: Keep-alive + installs (run once — prevents disconnects)\n",
        "from IPython.display import display, Javascript\n",
        "display(Javascript('''\n",
        "function ClickConnect(){\n",
        "  console.log(\"Keeping alive\");\n",
        "  document.querySelector(\"colab-connect-button\")?.click()\n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "'''))\n",
        "print(\"Keep-alive activated — no disconnect curse\")\n",
        "\n",
        "!pip install torch matplotlib numpy\n",
        "\n",
        "# Second cell: The sim code (optimized — 3000 epochs, checkpoints, progress prints + visualization)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.amp\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "import numpy as np\n",
        "from contextlib import nullcontext\n",
        "import math\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import time  # for timing\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# CONFIG – optimized for speed + stability\n",
        "triality = 3\n",
        "dim = 384\n",
        "latent_dim = 8\n",
        "seq_len = 1024  # sequence length\n",
        "batch_size = 64\n",
        "epochs = 3000  # reduced for fast run (sigma trend visible early)\n",
        "lr = 5e-5\n",
        "use_amp = True\n",
        "use_checkpoint = True\n",
        "\n",
        "checkpoint_dir = \"./checkpoints\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "checkpoint_path = os.path.join(checkpoint_dir, \"data_poisoning_checkpoint.pth\")\n",
        "\n",
        "# Synthetic AI data poisoning proxy (clean sequences + poisoned examples + noise/occlusion)\n",
        "features_seq = 128\n",
        "\n",
        "# Clean target sequences\n",
        "clean_target = []\n",
        "for b in range(batch_size):\n",
        "    t = torch.linspace(0, 10*math.pi, seq_len, device=device)\n",
        "    clean = torch.sin(t.unsqueeze(-1) * torch.arange(features_seq, device=device)) * 0.5\n",
        "    clean_target.append(clean)\n",
        "clean_target = torch.stack(clean_target).to(device)\n",
        "\n",
        "# Poisoned data (subset with flipped patterns + noise)\n",
        "poisoned_data = []\n",
        "poison_ratio = 0.2  # 20% poisoned\n",
        "for b in range(batch_size):\n",
        "    t = torch.linspace(0, 10*math.pi, seq_len, device=device)\n",
        "    base = torch.sin(t.unsqueeze(-1) * torch.arange(features_seq, device=device)) * 0.5\n",
        "\n",
        "    # Poison injection (flip sign on subset)\n",
        "    if b < batch_size * poison_ratio:\n",
        "        base = -base  # poisoned flip\n",
        "\n",
        "    base += torch.randn_like(base) * 0.15  # noise\n",
        "\n",
        "    poisoned_data.append(base)\n",
        "\n",
        "poisoned_data = torch.stack(poisoned_data).to(device)\n",
        "\n",
        "# Project to shared dim\n",
        "proj = nn.Linear(features_seq, dim).to(device)\n",
        "clean_data = proj(clean_target)\n",
        "poisoned_data = proj(poisoned_data)\n",
        "\n",
        "# High masking (70–90% — additional occlusion proxy)\n",
        "missing_rate = torch.linspace(0.7, 0.9, batch_size, device=device).view(batch_size, 1, 1)\n",
        "mask = torch.rand_like(poisoned_data) < missing_rate\n",
        "real_data = poisoned_data.clone()\n",
        "real_data[mask] = 0\n",
        "\n",
        "target = clean_data  # goal: learn clean despite poison\n",
        "\n",
        "# E8 roots – precompute\n",
        "def get_e8_roots():\n",
        "    roots = []\n",
        "    for i in range(8):\n",
        "        for j in range(i+1, 8):\n",
        "            for signs in [(1,1), (1,-1), (-1,1), (-1,-1)]:\n",
        "                v = torch.zeros(8)\n",
        "                v[i] = signs[0]; v[j] = signs[1]\n",
        "                roots.append(v); roots.append(-v)\n",
        "    for signs in range(1 << 8):\n",
        "        v = torch.tensor([(1 if (signs & (1<<k)) else -1) for k in range(8)], dtype=torch.float32) * 0.5\n",
        "        if bin(signs).count('1') % 2 == 0:\n",
        "            roots.append(v); roots.append(-v)\n",
        "    roots = torch.stack(roots[:240])\n",
        "    return roots / roots.norm(dim=-1, keepdim=True)\n",
        "\n",
        "e8_roots = get_e8_roots().to(device)\n",
        "\n",
        "# Triality Cycle Block\n",
        "class PoisonCycleBlock(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(latent_dim, dim // triality, bias=False)\n",
        "        self.register_buffer('roots', e8_roots)\n",
        "\n",
        "    def forward(self, x, step):\n",
        "        pos_emb = self.roots[torch.arange(x.shape[1], device=device) % 240]\n",
        "        low_dim = self.proj(pos_emb)\n",
        "        emb = low_dim.repeat(1, triality)\n",
        "        with torch.no_grad():\n",
        "            pump_scalar = 0.8 * math.sin(step * 0.006 * 2 * math.pi)\n",
        "        pump = torch.full((1, x.shape[1], 1), pump_scalar, device=device)\n",
        "        emb_broadcast = emb.unsqueeze(0)\n",
        "        x_rot1 = x * (emb_broadcast.cos() + pump)\n",
        "        x_rot2 = torch.roll(x_rot1, shifts=1, dims=1) * emb_broadcast.sin()\n",
        "        x_rot3 = torch.roll(x_rot2, shifts=1, dims=1) * emb_broadcast.cos()\n",
        "        fused = (x_rot1 + x_rot2 + x_rot3) / triality\n",
        "        return fused\n",
        "\n",
        "# Dummy cycle for ablation\n",
        "class DummyCycle(nn.Module):\n",
        "    def forward(self, x, step=None):\n",
        "        return x\n",
        "\n",
        "# Model with ablation support\n",
        "class E8PoisonFusion(nn.Module):\n",
        "    def __init__(self, depth=32, use_triality=True):\n",
        "        super().__init__()\n",
        "        self.use_triality = use_triality\n",
        "        self.cycle = PoisonCycleBlock() if use_triality else DummyCycle()\n",
        "        self.layers = nn.ModuleList([nn.MultiheadAttention(dim, triality if use_triality else 8, batch_first=True) for _ in range(depth)])\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.head = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x, step):\n",
        "        x = self.cycle(x, step)\n",
        "        for layer in self.layers:\n",
        "            if use_checkpoint:\n",
        "                attn, _ = checkpoint(layer, x, x, x, use_reentrant=False)\n",
        "            else:\n",
        "                attn, _ = layer(x, x, x)\n",
        "            x = x + attn\n",
        "            x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "# Models\n",
        "model = E8PoisonFusion(use_triality=True).to(device)\n",
        "model_ablation = E8PoisonFusion(use_triality=False).to(device)\n",
        "\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "scaler = torch.amp.GradScaler('cuda') if use_amp else nullcontext()\n",
        "\n",
        "opt_ablation = torch.optim.AdamW(model_ablation.parameters(), lr=lr)\n",
        "scaler_ablation = torch.amp.GradScaler('cuda') if use_amp else nullcontext()\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "loss_hist = []\n",
        "loss_abl_hist = []\n",
        "\n",
        "start_epoch = 0\n",
        "start_time = time.time()\n",
        "\n",
        "# Load checkpoint if exists (resume on disconnect)\n",
        "if os.path.exists(checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['model'])\n",
        "    model_ablation.load_state_dict(checkpoint['model_ablation'])\n",
        "    opt.load_state_dict(checkpoint['opt'])\n",
        "    opt_ablation.load_state_dict(checkpoint['opt_ablation'])\n",
        "    scaler.load_state_dict(checkpoint['scaler'])\n",
        "    scaler_ablation.load_state_dict(checkpoint['scaler_ablation'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    loss_hist = checkpoint['loss_hist']\n",
        "    loss_abl_hist = checkpoint['loss_abl_hist']\n",
        "    print(f\"Resumed from epoch {start_epoch}\")\n",
        "\n",
        "for epoch in range(start_epoch, epochs):\n",
        "    epoch_start = time.time()\n",
        "    opt.zero_grad(set_to_none=True)\n",
        "    opt_ablation.zero_grad(set_to_none=True)\n",
        "\n",
        "    with torch.amp.autocast(device_type='cuda', dtype=torch.float16) if use_amp else nullcontext():\n",
        "        recon = model(real_data, epoch)\n",
        "        loss = loss_fn(recon, target)\n",
        "\n",
        "        recon_abl = model_ablation(real_data, epoch)\n",
        "        loss_abl = loss_fn(recon_abl, target)\n",
        "\n",
        "    scaler.scale(loss).backward() if use_amp else loss.backward()\n",
        "    scaler.unscale_(opt) if use_amp else None\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1e6)\n",
        "    scaler.step(opt) if use_amp else opt.step()\n",
        "    scaler.update() if use_amp else None\n",
        "\n",
        "    scaler_ablation.scale(loss_abl).backward() if use_amp else loss_abl.backward()\n",
        "    scaler_ablation.unscale_(opt_ablation) if use_amp else None\n",
        "    torch.nn.utils.clip_grad_norm_(model_ablation.parameters(), 1e6)\n",
        "    scaler_ablation.step(opt_ablation) if use_amp else opt_ablation.step()\n",
        "    scaler_ablation.update() if use_amp else None\n",
        "\n",
        "    loss_hist.append(loss.item())\n",
        "    loss_abl_hist.append(loss_abl.item())\n",
        "\n",
        "    epoch_time = time.time() - epoch_start\n",
        "    remaining_epochs = epochs - (epoch + 1)\n",
        "    estimated_remaining = remaining_epochs * epoch_time / 3600  # hours\n",
        "\n",
        "    if epoch % 500 == 0:\n",
        "        print(f\"Epoch {epoch} | Triality Loss {loss.item():.6f} | Ablation Loss {loss_abl.item():.6f} | Time per epoch: {epoch_time:.2f}s | Estimated remaining: {estimated_remaining:.2f} hours\")\n",
        "\n",
        "    # Checkpoint every 1000 epochs\n",
        "    if (epoch + 1) % 1000 == 0:\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model': model.state_dict(),\n",
        "            'model_ablation': model_ablation.state_dict(),\n",
        "            'opt': opt.state_dict(),\n",
        "            'opt_ablation': opt_ablation.state_dict(),\n",
        "            'scaler': scaler.state_dict(),\n",
        "            'scaler_ablation': scaler_ablation.state_dict(),\n",
        "            'loss_hist': loss_hist,\n",
        "            'loss_abl_hist': loss_abl_hist,\n",
        "        }, checkpoint_path)\n",
        "        print(f\"Checkpoint saved at epoch {epoch}\")\n",
        "\n",
        "# Final Sigma Test\n",
        "triality_mean = np.mean(loss_hist)\n",
        "abl_mean = np.mean(loss_abl_hist)\n",
        "std = np.std(loss_hist + loss_abl_hist)\n",
        "sigma = (abl_mean - triality_mean) / std if std > 0 else 0\n",
        "\n",
        "print(f\"Final Sigma (Triality vs Ablation): {sigma:.2f} (higher = triality advantage)\")\n",
        "\n",
        "# Visualization: Sequence Reconstruction (first feature channel proxy)\n",
        "model.eval()\n",
        "model_ablation.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Fresh test batch for viz (with poison)\n",
        "    test_data = []\n",
        "    for b in range(8):\n",
        "        t = torch.linspace(0, 10*math.pi, seq_len, device=device)\n",
        "        base = torch.sin(t.unsqueeze(-1) * torch.arange(features_seq, device=device)) * 0.5\n",
        "        # Poison on half\n",
        "        if b < 4:\n",
        "            base = -base  # poisoned\n",
        "        base += torch.randn_like(base) * 0.15\n",
        "        test_data.append(base)\n",
        "    test_data = torch.stack(test_data).to(device)\n",
        "\n",
        "    clean = proj(torch.stack([torch.sin(t.unsqueeze(-1) * torch.arange(features_seq, device=device)) * 0.5 for _ in range(8)]).to(device))\n",
        "    poisoned = proj(test_data)\n",
        "\n",
        "    mask = torch.rand_like(poisoned) < 0.8\n",
        "    masked = poisoned.clone()\n",
        "    masked[mask] = 0\n",
        "\n",
        "    recon = model(masked, 0)\n",
        "    recon_abl = model_ablation(masked, 0)\n",
        "\n",
        "    # Plot first sequence feature channel\n",
        "    orig = clean.cpu().numpy()[:, :, 0]\n",
        "    poisoned_plot = poisoned.cpu().numpy()[:, :, 0]\n",
        "    tri = recon.cpu().numpy()[:, :, 0]\n",
        "    abl = recon_abl.cpu().numpy()[:, :, 0]\n",
        "\n",
        "    fig, axes = plt.subplots(4, 8, figsize=(16, 8))\n",
        "    for i in range(8):\n",
        "        axes[0, i].plot(orig[i])\n",
        "        axes[0, i].set_title(\"Clean Sequence\")\n",
        "        axes[1, i].plot(poisoned_plot[i])\n",
        "        axes[1, i].set_title(\"Poisoned Input\")\n",
        "        axes[2, i].plot(tri[i])\n",
        "        axes[2, i].set_title(\"Triality Defense\")\n",
        "        axes[3, i].plot(abl[i])\n",
        "        axes[3, i].set_title(\"Ablation (Poisoned)\")\n",
        "    plt.suptitle(\"E8 Triality AI Data Poisoning Defense Visualization\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"Sim complete — visualization displayed!\")"
      ]
    }
  ]
}