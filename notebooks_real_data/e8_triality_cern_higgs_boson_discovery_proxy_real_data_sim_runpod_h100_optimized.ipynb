{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5U1D2hvyb2o",
        "outputId": "420e8ad9-5dfa-4dd5-8b09-ebb3f6d7027d"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Epoch 0 | Triality Loss 33.365021 | Ablation Loss 29.105597\n",
            "Epoch 500 | Triality Loss 0.027632 | Ablation Loss 0.051425\n",
            "Epoch 1000 | Triality Loss 0.020081 | Ablation Loss 0.046025\n",
            "Epoch 1500 | Triality Loss 0.017011 | Ablation Loss 0.042758\n",
            "Epoch 2000 | Triality Loss 0.015674 | Ablation Loss 0.041308\n",
            "Epoch 2500 | Triality Loss 0.014785 | Ablation Loss 0.040499\n",
            "Epoch 3000 | Triality Loss 0.014335 | Ablation Loss 0.039508\n",
            "Epoch 3500 | Triality Loss 0.013421 | Ablation Loss 0.039827\n",
            "Epoch 4000 | Triality Loss 0.013730 | Ablation Loss 0.039365\n",
            "Epoch 4500 | Triality Loss 0.013257 | Ablation Loss 0.038801\n",
            "Epoch 5000 | Triality Loss 0.013237 | Ablation Loss 0.037974\n",
            "Epoch 5500 | Triality Loss 0.012789 | Ablation Loss 0.037736\n",
            "Epoch 6000 | Triality Loss 0.012404 | Ablation Loss 0.036622\n",
            "Epoch 6500 | Triality Loss 0.011856 | Ablation Loss 0.035824\n",
            "Epoch 7000 | Triality Loss 0.011261 | Ablation Loss 0.034985\n",
            "Epoch 7500 | Triality Loss 0.010485 | Ablation Loss 0.034269\n",
            "Epoch 8000 | Triality Loss 0.009702 | Ablation Loss 0.033241\n",
            "Epoch 8500 | Triality Loss 0.009028 | Ablation Loss 0.031839\n",
            "Epoch 9000 | Triality Loss 0.008262 | Ablation Loss 0.029493\n",
            "Epoch 9500 | Triality Loss 0.007872 | Ablation Loss 0.026738\n",
            "Epoch 10000 | Triality Loss 0.007546 | Ablation Loss 0.023798\n",
            "Epoch 10500 | Triality Loss 0.007282 | Ablation Loss 0.021614\n",
            "Epoch 11000 | Triality Loss 0.007101 | Ablation Loss 0.019412\n",
            "Epoch 11500 | Triality Loss 0.006864 | Ablation Loss 0.017453\n",
            "Epoch 12000 | Triality Loss 0.006730 | Ablation Loss 0.015876\n",
            "Epoch 12500 | Triality Loss 0.006595 | Ablation Loss 0.014465\n",
            "Epoch 13000 | Triality Loss 0.006413 | Ablation Loss 0.013311\n",
            "Epoch 13500 | Triality Loss 0.006270 | Ablation Loss 0.012434\n",
            "Epoch 14000 | Triality Loss 0.006089 | Ablation Loss 0.011555\n",
            "Epoch 14500 | Triality Loss 0.005977 | Ablation Loss 0.010893\n",
            "Epoch 15000 | Triality Loss 0.005845 | Ablation Loss 0.010266\n",
            "Epoch 15500 | Triality Loss 0.005775 | Ablation Loss 0.009865\n",
            "Epoch 16000 | Triality Loss 0.005687 | Ablation Loss 0.009464\n",
            "Epoch 16500 | Triality Loss 0.005620 | Ablation Loss 0.009138\n",
            "Epoch 17000 | Triality Loss 0.005558 | Ablation Loss 0.008852\n",
            "Epoch 17500 | Triality Loss 0.005524 | Ablation Loss 0.008712\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.amp\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "import numpy as np\n",
        "from contextlib import nullcontext\n",
        "import math\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# CONFIG – optimized for H100 speed (fast epochs)\n",
        "triality = 3\n",
        "dim = 384\n",
        "latent_dim = 8\n",
        "seq_len = 1024  # collision \"hits\" proxy\n",
        "batch_size = 32\n",
        "epochs = 20000\n",
        "lr = 5e-5\n",
        "use_amp = True\n",
        "use_checkpoint = True\n",
        "\n",
        "# Synthetic Higgs boson proxy (4-lepton golden channel + background noise/occlusion)\n",
        "# Real-like curved helical tracks for signal + random hits for background\n",
        "events = []\n",
        "for b in range(batch_size):\n",
        "    # Signal: 4 leptons (curved paths in magnetic field proxy)\n",
        "    t = torch.linspace(0, 2*math.pi, seq_len, device=device)\n",
        "    lepton1 = torch.stack([torch.sin(t), torch.cos(t), t/10], dim=-1) * 50\n",
        "    lepton2 = torch.stack([torch.sin(t + math.pi/2), torch.cos(t + math.pi/2), t/10], dim=-1) * 40\n",
        "    lepton3 = torch.stack([torch.sin(t + math.pi), torch.cos(t + math.pi), t/10], dim=-1) * 30\n",
        "    lepton4 = torch.stack([torch.sin(t + 3*math.pi/2), torch.cos(t + 3*math.pi/2), t/10], dim=-1) * 20\n",
        "\n",
        "    # Stack leptons + background noise\n",
        "    signal = torch.cat([lepton1, lepton2, lepton3, lepton4], dim=-1)  # (seq_len, 12 features proxy)\n",
        "    background = torch.randn_like(signal) * 5  # noise\n",
        "    event = signal + background\n",
        "\n",
        "    # Normalize\n",
        "    event = (event - event.min()) / (event.max() - event.min() + 1e-6)\n",
        "    events.append(event)\n",
        "\n",
        "events = torch.stack(events).to(device)  # (batch, seq_len, features)\n",
        "\n",
        "# Project to dim\n",
        "proj = nn.Linear(events.shape[-1], dim).to(device)\n",
        "\n",
        "# E8 roots – precompute\n",
        "def get_e8_roots():\n",
        "    roots = []\n",
        "    for i in range(8):\n",
        "        for j in range(i+1, 8):\n",
        "            for signs in [(1,1), (1,-1), (-1,1), (-1,-1)]:\n",
        "                v = torch.zeros(8)\n",
        "                v[i] = signs[0]; v[j] = signs[1]\n",
        "                roots.append(v); roots.append(-v)\n",
        "    for signs in range(1 << 8):\n",
        "        v = torch.tensor([(1 if (signs & (1<<k)) else -1) for k in range(8)], dtype=torch.float32) * 0.5\n",
        "        if bin(signs).count('1') % 2 == 0:\n",
        "            roots.append(v); roots.append(-v)\n",
        "    roots = torch.stack(roots[:240])\n",
        "    return roots / roots.norm(dim=-1, keepdim=True)\n",
        "\n",
        "e8_roots = get_e8_roots().to(device)\n",
        "\n",
        "# Triality Cycle Block (detached pump scalar)\n",
        "class HiggsCycleBlock(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(latent_dim, dim // triality, bias=False)\n",
        "        self.register_buffer('roots', e8_roots)\n",
        "\n",
        "    def forward(self, x, step):\n",
        "        pos_emb = self.roots[torch.arange(x.shape[1], device=device) % 240]\n",
        "        low_dim = self.proj(pos_emb)\n",
        "        emb = low_dim.repeat(1, triality)\n",
        "        with torch.no_grad():\n",
        "            pump_scalar = 0.8 * math.sin(step * 0.006 * 2 * math.pi)\n",
        "        pump = torch.full((1, x.shape[1], 1), pump_scalar, device=device)\n",
        "        emb_broadcast = emb.unsqueeze(0)\n",
        "        x_rot1 = x * (emb_broadcast.cos() + pump)\n",
        "        x_rot2 = torch.roll(x_rot1, shifts=1, dims=1) * emb_broadcast.sin()\n",
        "        x_rot3 = torch.roll(x_rot2, shifts=1, dims=1) * emb_broadcast.cos()\n",
        "        fused = (x_rot1 + x_rot2 + x_rot3) / triality\n",
        "        return fused\n",
        "\n",
        "# Dummy cycle for ablation\n",
        "class DummyCycle(nn.Module):\n",
        "    def forward(self, x, step=None):\n",
        "        return x\n",
        "\n",
        "# Model with ablation support\n",
        "class E8HiggsFusion(nn.Module):\n",
        "    def __init__(self, depth=32, use_triality=True):\n",
        "        super().__init__()\n",
        "        self.use_triality = use_triality\n",
        "        self.cycle = HiggsCycleBlock() if use_triality else DummyCycle()\n",
        "        self.layers = nn.ModuleList([nn.MultiheadAttention(dim, triality if use_triality else 8, batch_first=True) for _ in range(depth)])\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.head = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x, step):\n",
        "        x = self.cycle(x, step)\n",
        "        for layer in self.layers:\n",
        "            if use_checkpoint:\n",
        "                attn, _ = checkpoint(layer, x, x, x, use_reentrant=False)\n",
        "            else:\n",
        "                attn, _ = layer(x, x, x)\n",
        "            x = x + self.norm(attn)\n",
        "        return x\n",
        "\n",
        "# Models\n",
        "model = E8HiggsFusion(use_triality=True).to(device)\n",
        "model_ablation = E8HiggsFusion(use_triality=False).to(device)\n",
        "\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "sch_model = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\n",
        "sch_model_step_after_opt = True # Flag to ensure scheduler step is called correctly\n",
        "scaler = torch.amp.GradScaler('cuda') if use_amp else nullcontext()\n",
        "\n",
        "opt_ablation = torch.optim.AdamW(model_ablation.parameters(), lr=lr)\n",
        "sch_ablation = torch.optim.lr_scheduler.CosineAnnealingLR(opt_ablation, T_max=epochs)\n",
        "sch_ablation_step_after_opt = True # Flag to ensure scheduler step is called correctly\n",
        "scaler_ablation = torch.amp.GradScaler('cuda') if use_amp else nullcontext()\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "loss_hist = []\n",
        "loss_abl_hist = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Re-compute clean_data and real_data each epoch to ensure fresh graph for backprop\n",
        "    clean_data = proj(events)\n",
        "    missing_rate = torch.linspace(0.4, 0.7, batch_size, device=device).view(batch_size, 1, 1)\n",
        "    mask = torch.rand_like(clean_data) < missing_rate\n",
        "    real_data = clean_data.clone().masked_fill(mask, 0)\n",
        "    target = clean_data\n",
        "\n",
        "    opt.zero_grad(set_to_none=True)\n",
        "    opt_ablation.zero_grad(set_to_none=True)\n",
        "\n",
        "    with torch.amp.autocast(device_type='cuda', dtype=torch.float16) if use_amp else nullcontext():\n",
        "        recon = model(real_data, epoch)\n",
        "        loss = loss_fn(recon, target)\n",
        "\n",
        "        recon_abl = model_ablation(real_data, epoch)\n",
        "        loss_abl = loss_fn(recon_abl, target)\n",
        "\n",
        "    scaler.scale(loss).backward(retain_graph=True) if use_amp else loss.backward(retain_graph=True)\n",
        "    scaler.unscale_(opt) if use_amp else None\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1e6)\n",
        "    scaler.step(opt) if use_amp else opt.step()\n",
        "    if sch_model_step_after_opt: # Only step the scheduler after the optimizer\n",
        "      sch_model.step()\n",
        "    scaler.update() if use_amp else None\n",
        "\n",
        "    scaler_ablation.scale(loss_abl).backward() if use_amp else loss_abl.backward()\n",
        "    scaler_ablation.unscale_(opt_ablation) if use_amp else None\n",
        "    torch.nn.utils.clip_grad_norm_(model_ablation.parameters(), 1e6)\n",
        "    scaler_ablation.step(opt_ablation) if use_amp else opt_ablation.step()\n",
        "    if sch_ablation_step_after_opt: # Only step the scheduler after the optimizer\n",
        "      sch_ablation.step()\n",
        "    scaler_ablation.update() if use_amp else None\n",
        "\n",
        "    loss_hist.append(loss.item())\n",
        "    loss_abl_hist.append(loss_abl.item())\n",
        "\n",
        "    if epoch % 500 == 0:\n",
        "        print(f\"Epoch {epoch} | Triality Loss {loss.item():.6f} | Ablation Loss {loss_abl.item():.6f}\")\n",
        "\n",
        "# Final Sigma Test\n",
        "triality_mean = np.mean(loss_hist)\n",
        "abl_mean = np.mean(loss_abl_hist)\n",
        "std = np.std(loss_hist + loss_abl_hist)\n",
        "sigma = (abl_mean - triality_mean) / std if std > 0 else 0\n",
        "\n",
        "print(f\"Final Sigma (Triality vs Ablation): {sigma:.2f} (higher = triality advantage)\")\n",
        "\n",
        "print(\"Sim complete — epochs + sigma test done\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "H100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}