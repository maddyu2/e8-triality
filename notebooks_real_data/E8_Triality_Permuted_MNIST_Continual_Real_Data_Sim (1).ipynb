{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# First cell: Keep-alive + installs (run once — prevents disconnects)\n",
        "from IPython.display import display, Javascript\n",
        "display(Javascript('''\n",
        "function ClickConnect(){\n",
        "  console.log(\"Keeping alive\");\n",
        "  document.querySelector(\"colab-connect-button\")?.click()\n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "'''))\n",
        "print(\"Keep-alive activated — no disconnect curse\")\n",
        "\n",
        "!pip install torch matplotlib numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "id": "anU2OJx291-g",
        "outputId": "771198e9-2dc2-4213-a543-74162595e44a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "function ClickConnect(){\n",
              "  console.log(\"Keeping alive\"); \n",
              "  document.querySelector(\"colab-connect-button\")?.click()\n",
              "}\n",
              "setInterval(ClickConnect,60000)\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keep-alive activated — no disconnect curse\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (26.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Second cell: The sim code (optimized — 5000 epochs, larger batch, single backward)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.amp\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "import numpy as np\n",
        "from contextlib import nullcontext\n",
        "import math\n",
        "import os\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# CONFIG – optimized for speed + stability\n",
        "triality = 3\n",
        "dim = 384\n",
        "latent_dim = 8\n",
        "seq_len = 1024  # time steps (robot actions/frames)\n",
        "batch_size = 64\n",
        "epochs = 5000  # reduced for fast run (sigma trend visible early)\n",
        "lr = 5e-5\n",
        "use_amp = True\n",
        "use_checkpoint = True\n",
        "\n",
        "checkpoint_dir = \"./checkpoints\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "checkpoint_path = os.path.join(checkpoint_dir, \"proprio_vision_checkpoint.pth\")\n",
        "\n",
        "# Synthetic robotics proprio + vision fusion proxy (proprioceptive states + vision features + noise/occlusion)\n",
        "features_proprio = 128  # joint angles/torques\n",
        "features_vision = 128   # camera features\n",
        "\n",
        "robot_data = []\n",
        "for b in range(batch_size):\n",
        "    t = torch.linspace(0, 10*math.pi, seq_len, device=device)\n",
        "\n",
        "    proprio = torch.sin(t.unsqueeze(-1) * torch.arange(features_proprio, device=device)) * 0.5  # internal state\n",
        "    vision = torch.cos(t.unsqueeze(-1) * torch.arange(features_vision, device=device) * 1.2) * 0.4   # visual motion\n",
        "\n",
        "    frame = torch.cat([proprio, vision], dim=-1)\n",
        "    frame += torch.randn_like(frame) * 0.1  # noise\n",
        "    robot_data.append(frame)\n",
        "\n",
        "robot_data = torch.stack(robot_data).to(device)\n",
        "\n",
        "# Project to shared dim\n",
        "proj = nn.Linear(features_proprio + features_vision, dim).to(device)\n",
        "clean_data = proj(robot_data)\n",
        "\n",
        "# High masking (70–90% — sensor dropout/occlusion proxy)\n",
        "missing_rate = torch.linspace(0.7, 0.9, batch_size, device=device).view(batch_size, 1, 1)\n",
        "mask = torch.rand_like(clean_data) < missing_rate\n",
        "real_data = clean_data.clone()\n",
        "real_data[mask] = 0\n",
        "\n",
        "target = clean_data\n",
        "\n",
        "# E8 roots – precompute\n",
        "def get_e8_roots():\n",
        "    roots = []\n",
        "    for i in range(8):\n",
        "        for j in range(i+1, 8):\n",
        "            for signs in [(1,1), (1,-1), (-1,1), (-1,-1)]:\n",
        "                v = torch.zeros(8)\n",
        "                v[i] = signs[0]; v[j] = signs[1]\n",
        "                roots.append(v); roots.append(-v)\n",
        "    for signs in range(1 << 8):\n",
        "        v = torch.tensor([(1 if (signs & (1<<k)) else -1) for k in range(8)], dtype=torch.float32) * 0.5\n",
        "        if bin(signs).count('1') % 2 == 0:\n",
        "            roots.append(v); roots.append(-v)\n",
        "    roots = torch.stack(roots[:240])\n",
        "    return roots / roots.norm(dim=-1, keepdim=True)\n",
        "\n",
        "e8_roots = get_e8_roots().to(device)\n",
        "\n",
        "# Triality Cycle Block (detached pump scalar)\n",
        "class RoboticsCycleBlock(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(latent_dim, dim // triality, bias=False)\n",
        "        self.register_buffer('roots', e8_roots)\n",
        "\n",
        "    def forward(self, x, step):\n",
        "        pos_emb = self.roots[torch.arange(x.shape[1], device=device) % 240]\n",
        "        low_dim = self.proj(pos_emb)\n",
        "        emb = low_dim.repeat(1, triality)\n",
        "        with torch.no_grad():\n",
        "            pump_scalar = 0.8 * math.sin(step * 0.006 * 2 * math.pi)\n",
        "        pump = torch.full((1, x.shape[1], 1), pump_scalar, device=device)\n",
        "        emb_broadcast = emb.unsqueeze(0)\n",
        "        x_rot1 = x * (emb_broadcast.cos() + pump)\n",
        "        x_rot2 = torch.roll(x_rot1, shifts=1, dims=1) * emb_broadcast.sin()\n",
        "        x_rot3 = torch.roll(x_rot2, shifts=1, dims=1) * emb_broadcast.cos()\n",
        "        fused = (x_rot1 + x_rot2 + x_rot3) / triality\n",
        "        return fused\n",
        "\n",
        "# Dummy cycle for ablation\n",
        "class DummyCycle(nn.Module):\n",
        "    def forward(self, x, step=None):\n",
        "        return x\n",
        "\n",
        "# Model with ablation support\n",
        "class E8RoboticsFusion(nn.Module):\n",
        "    def __init__(self, depth=32, use_triality=True):\n",
        "        super().__init__()\n",
        "        self.use_triality = use_triality\n",
        "        self.cycle = RoboticsCycleBlock() if use_triality else DummyCycle()\n",
        "        self.layers = nn.ModuleList([nn.MultiheadAttention(dim, triality if use_triality else 8, batch_first=True) for _ in range(depth)])\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.head = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x, step):\n",
        "        x = self.cycle(x, step)\n",
        "        for layer in self.layers:\n",
        "            if use_checkpoint:\n",
        "                attn, _ = checkpoint(layer, x, x, x, use_reentrant=False)\n",
        "            else:\n",
        "                attn, _ = layer(x, x, x)\n",
        "            x = x + attn\n",
        "            x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "# Models\n",
        "model = E8RoboticsFusion(use_triality=True).to(device)\n",
        "model_ablation = E8RoboticsFusion(use_triality=False).to(device)\n",
        "\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "scaler = torch.amp.GradScaler('cuda') if use_amp else nullcontext()\n",
        "\n",
        "opt_ablation = torch.optim.AdamW(model_ablation.parameters(), lr=lr)\n",
        "scaler_ablation = torch.amp.GradScaler('cuda') if use_amp else nullcontext()\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "loss_hist = []\n",
        "loss_abl_hist = []\n",
        "\n",
        "start_epoch = 0\n",
        "\n",
        "# Load checkpoint if exists (resume on disconnect)\n",
        "if os.path.exists(checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['model'])\n",
        "    model_ablation.load_state_dict(checkpoint['model_ablation'])\n",
        "    opt.load_state_dict(checkpoint['opt'])\n",
        "    opt_ablation.load_state_dict(checkpoint['opt_ablation'])\n",
        "    scaler.load_state_dict(checkpoint['scaler'])\n",
        "    scaler_ablation.load_state_dict(checkpoint['scaler_ablation'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    loss_hist = checkpoint['loss_hist']\n",
        "    loss_abl_hist = checkpoint['loss_abl_hist']\n",
        "    print(f\"Resumed from epoch {start_epoch}\")\n",
        "\n",
        "for epoch in range(start_epoch, epochs):\n",
        "    opt.zero_grad(set_to_none=True)\n",
        "    opt_ablation.zero_grad(set_to_none=True)\n",
        "\n",
        "    with torch.amp.autocast(device_type='cuda', dtype=torch.float16) if use_amp else nullcontext():\n",
        "        recon = model(real_data, epoch)\n",
        "        loss = loss_fn(recon, target)\n",
        "\n",
        "        recon_abl = model_ablation(real_data, epoch)\n",
        "        loss_abl = loss_fn(recon_abl, target)\n",
        "\n",
        "    scaler.scale(loss).backward(retain_graph=True) if use_amp else loss.backward(retain_graph=True)\n",
        "    scaler.unscale_(opt) if use_amp else None\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1e6)\n",
        "    scaler.step(opt) if use_amp else opt.step()\n",
        "    scaler.update() if use_amp else None\n",
        "\n",
        "    scaler_ablation.scale(loss_abl).backward() if use_amp else loss_abl.backward()\n",
        "    scaler_ablation.unscale_(opt_ablation) if use_amp else None\n",
        "    torch.nn.utils.clip_grad_norm_(model_ablation.parameters(), 1e6)\n",
        "    scaler_ablation.step(opt_ablation) if use_amp else opt_ablation.step()\n",
        "    scaler_ablation.update() if use_amp else None\n",
        "\n",
        "    loss_hist.append(loss.item())\n",
        "    loss_abl_hist.append(loss_abl.item())\n",
        "\n",
        "    if epoch % 500 == 0:\n",
        "        print(f\"Epoch {epoch} | Triality Loss {loss.item():.6f} | Ablation Loss {loss_abl.item():.6f}\")\n",
        "\n",
        "    # Checkpoint every 1000 epochs\n",
        "    if (epoch + 1) % 1000 == 0:\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model': model.state_dict(),\n",
        "            'model_ablation': model_ablation.state_dict(),\n",
        "            'opt': opt.state_dict(),\n",
        "            'opt_ablation': opt_ablation.state_dict(),\n",
        "            'scaler': scaler.state_dict(),\n",
        "            'scaler_ablation': scaler_ablation.state_dict(),\n",
        "            'loss_hist': loss_hist,\n",
        "            'loss_abl_hist': loss_abl_hist,\n",
        "        }, checkpoint_path)\n",
        "        print(f\"Checkpoint saved at epoch {epoch}\")\n",
        "\n",
        "# Final Sigma Test\n",
        "triality_mean = np.mean(loss_hist)\n",
        "abl_mean = np.mean(loss_abl_hist)\n",
        "std = np.std(loss_hist + loss_abl_hist)\n",
        "sigma = (abl_mean - triality_mean) / std if std > 0 else 0\n",
        "\n",
        "print(f\"Final Sigma (Triality vs Ablation): {sigma:.2f} (higher = triality advantage)\")\n",
        "\n",
        "print(\"Sim complete — epochs + sigma test done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "0oJjrb1z93nm",
        "outputId": "205ec6a3-5cb4-45f2-9bd5-8e77be465818"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Epoch 0 | Triality Loss 0.924989 | Ablation Loss 0.914548\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1808573827.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mloss_abl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_abl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_amp\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m     \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_amp\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cfafb99"
      },
      "source": [
        "# Task\n",
        "Resolve the `RuntimeError` occurring during the training loop's second backward pass by adding `retain_graph=True` to the `scaler_ablation.scale(loss_abl).backward()` call, then execute the modified training loop to completion and provide an estimate for the total training time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7c9b80e"
      },
      "source": [
        "## Modify backward calls for graph retention\n",
        "\n",
        "### Subtask:\n",
        "Add `retain_graph=True` to the second backward call to ensure the computational graph is fully preserved for both optimization steps. This addresses the `RuntimeError` by explicitly instructing PyTorch to keep the graph even after the second backward pass, which might be necessary in complex scenarios involving checkpointing or AMP.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56063803"
      },
      "source": [
        "**Reasoning**:\n",
        "To resolve the `RuntimeError` related to backwarding through the graph a second time, I will add `retain_graph=True` to the `loss_abl.backward()` calls in the training loop. This ensures the computational graph remains available for both optimization steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7309c1b6",
        "outputId": "a73f210b-9cd6-46f4-acf6-e733468e5848"
      },
      "source": [
        "# Second cell: The sim code (optimized — 5000 epochs, larger batch, single backward)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.amp\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "import numpy as np\n",
        "from contextlib import nullcontext\n",
        "import math\n",
        "import os\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# CONFIG – optimized for speed + stability\n",
        "triality = 3\n",
        "dim = 384\n",
        "latent_dim = 8\n",
        "seq_len = 1024  # time steps (robot actions/frames)\n",
        "batch_size = 64\n",
        "epochs = 5000  # reduced for fast run (sigma trend visible early)\n",
        "lr = 5e-5\n",
        "use_amp = True\n",
        "use_checkpoint = True\n",
        "\n",
        "checkpoint_dir = \"./checkpoints\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "checkpoint_path = os.path.join(checkpoint_dir, \"proprio_vision_checkpoint.pth\")\n",
        "\n",
        "# Synthetic robotics proprio + vision fusion proxy (proprioceptive states + vision features + noise/occlusion)\n",
        "features_proprio = 128  # joint angles/torques\n",
        "features_vision = 128   # camera features\n",
        "\n",
        "robot_data = []\n",
        "for b in range(batch_size):\n",
        "    t = torch.linspace(0, 10*math.pi, seq_len, device=device)\n",
        "\n",
        "    proprio = torch.sin(t.unsqueeze(-1) * torch.arange(features_proprio, device=device)) * 0.5  # internal state\n",
        "    vision = torch.cos(t.unsqueeze(-1) * torch.arange(features_vision, device=device) * 1.2) * 0.4   # visual motion\n",
        "\n",
        "    frame = torch.cat([proprio, vision], dim=-1)\n",
        "    frame += torch.randn_like(frame) * 0.1  # noise\n",
        "    robot_data.append(frame)\n",
        "\n",
        "robot_data = torch.stack(robot_data).to(device)\n",
        "\n",
        "# Project to shared dim\n",
        "proj = nn.Linear(features_proprio + features_vision, dim).to(device)\n",
        "clean_data = proj(robot_data)\n",
        "\n",
        "# High masking (70–90% — sensor dropout/occlusion proxy)\n",
        "missing_rate = torch.linspace(0.7, 0.9, batch_size, device=device).view(batch_size, 1, 1)\n",
        "mask = torch.rand_like(clean_data) < missing_rate\n",
        "real_data = clean_data.clone()\n",
        "real_data[mask] = 0\n",
        "\n",
        "target = clean_data\n",
        "\n",
        "# E8 roots – precompute\n",
        "def get_e8_roots():\n",
        "    roots = []\n",
        "    for i in range(8):\n",
        "        for j in range(i+1, 8):\n",
        "            for signs in [(1,1), (1,-1), (-1,1), (-1,-1)]:\n",
        "                v = torch.zeros(8)\n",
        "                v[i] = signs[0]; v[j] = signs[1]\n",
        "                roots.append(v); roots.append(-v)\n",
        "    for signs in range(1 << 8):\n",
        "        v = torch.tensor([(1 if (signs & (1<<k)) else -1) for k in range(8)], dtype=torch.float32) * 0.5\n",
        "        if bin(signs).count('1') % 2 == 0:\n",
        "            roots.append(v); roots.append(-v)\n",
        "    roots = torch.stack(roots[:240])\n",
        "    return roots / roots.norm(dim=-1, keepdim=True)\n",
        "\n",
        "e8_roots = get_e8_roots().to(device)\n",
        "\n",
        "# Triality Cycle Block (detached pump scalar)\n",
        "class RoboticsCycleBlock(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(latent_dim, dim // triality, bias=False)\n",
        "        self.register_buffer('roots', e8_roots)\n",
        "\n",
        "    def forward(self, x, step):\n",
        "        pos_emb = self.roots[torch.arange(x.shape[1], device=device) % 240]\n",
        "        low_dim = self.proj(pos_emb)\n",
        "        emb = low_dim.repeat(1, triality)\n",
        "        with torch.no_grad():\n",
        "            pump_scalar = 0.8 * math.sin(step * 0.006 * 2 * math.pi)\n",
        "        pump = torch.full((1, x.shape[1], 1), pump_scalar, device=device)\n",
        "        emb_broadcast = emb.unsqueeze(0)\n",
        "        x_rot1 = x * (emb_broadcast.cos() + pump)\n",
        "        x_rot2 = torch.roll(x_rot1, shifts=1, dims=1) * emb_broadcast.sin()\n",
        "        x_rot3 = torch.roll(x_rot2, shifts=1, dims=1) * emb_broadcast.cos()\n",
        "        fused = (x_rot1 + x_rot2 + x_rot3) / triality\n",
        "        return fused\n",
        "\n",
        "# Dummy cycle for ablation\n",
        "class DummyCycle(nn.Module):\n",
        "    def forward(self, x, step=None):\n",
        "        return x\n",
        "\n",
        "# Model with ablation support\n",
        "class E8RoboticsFusion(nn.Module):\n",
        "    def __init__(self, depth=32, use_triality=True):\n",
        "        super().__init__()\n",
        "        self.use_triality = use_triality\n",
        "        self.cycle = RoboticsCycleBlock() if use_triality else DummyCycle()\n",
        "        self.layers = nn.ModuleList([nn.MultiheadAttention(dim, triality if use_triality else 8, batch_first=True) for _ in range(depth)])\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.head = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x, step):\n",
        "        x = self.cycle(x, step)\n",
        "        for layer in self.layers:\n",
        "            if use_checkpoint:\n",
        "                attn, _ = checkpoint(layer, x, x, x, use_reentrant=False)\n",
        "            else:\n",
        "                attn, _ = layer(x, x, x)\n",
        "            x = x + attn\n",
        "            x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "# Models\n",
        "model = E8RoboticsFusion(use_triality=True).to(device)\n",
        "model_ablation = E8RoboticsFusion(use_triality=False).to(device)\n",
        "\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "scaler = torch.amp.GradScaler('cuda') if use_amp else nullcontext()\n",
        "\n",
        "opt_ablation = torch.optim.AdamW(model_ablation.parameters(), lr=lr)\n",
        "scaler_ablation = torch.amp.GradScaler('cuda') if use_amp else nullcontext()\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "loss_hist = []\n",
        "loss_abl_hist = []\n",
        "\n",
        "start_epoch = 0\n",
        "\n",
        "# Load checkpoint if exists (resume on disconnect)\n",
        "if os.path.exists(checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['model'])\n",
        "    model_ablation.load_state_dict(checkpoint['model_ablation'])\n",
        "    opt.load_state_dict(checkpoint['opt'])\n",
        "    opt_ablation.load_state_dict(checkpoint['opt_ablation'])\n",
        "    scaler.load_state_dict(checkpoint['scaler'])\n",
        "    scaler_ablation.load_state_dict(checkpoint['scaler_ablation'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    loss_hist = checkpoint['loss_hist']\n",
        "    loss_abl_hist = checkpoint['loss_abl_hist']\n",
        "    print(f\"Resumed from epoch {start_epoch}\")\n",
        "\n",
        "for epoch in range(start_epoch, epochs):\n",
        "    opt.zero_grad(set_to_none=True)\n",
        "    opt_ablation.zero_grad(set_to_none=True)\n",
        "\n",
        "    with torch.amp.autocast(device_type='cuda', dtype=torch.float16) if use_amp else nullcontext():\n",
        "        recon = model(real_data, epoch)\n",
        "        loss = loss_fn(recon, target)\n",
        "\n",
        "        recon_abl = model_ablation(real_data, epoch)\n",
        "        loss_abl = loss_fn(recon_abl, target)\n",
        "\n",
        "    scaler.scale(loss).backward(retain_graph=True) if use_amp else loss.backward(retain_graph=True)\n",
        "    scaler.unscale_(opt) if use_amp else None\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1e6)\n",
        "    scaler.step(opt) if use_amp else opt.step()\n",
        "    scaler.update() if use_amp else None\n",
        "\n",
        "    scaler_ablation.scale(loss_abl).backward(retain_graph=True) if use_amp else loss_abl.backward(retain_graph=True)\n",
        "    scaler_ablation.unscale_(opt_ablation) if use_amp else None\n",
        "    torch.nn.utils.clip_grad_norm_(model_ablation.parameters(), 1e6)\n",
        "    scaler_ablation.step(opt_ablation) if use_amp else opt_ablation.step()\n",
        "    scaler_ablation.update() if use_amp else None\n",
        "\n",
        "    loss_hist.append(loss.item())\n",
        "    loss_abl_hist.append(loss_abl.item())\n",
        "\n",
        "    if epoch % 500 == 0:\n",
        "        print(f\"Epoch {epoch} | Triality Loss {loss.item():.6f} | Ablation Loss {loss_abl.item():.6f}\")\n",
        "\n",
        "    # Checkpoint every 1000 epochs\n",
        "    if (epoch + 1) % 1000 == 0:\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model': model.state_dict(),\n",
        "            'model_ablation': model_ablation.state_dict(),\n",
        "            'opt': opt.state_dict(),\n",
        "            'opt_ablation': opt_ablation.state_dict(),\n",
        "            'scaler': scaler.state_dict(),\n",
        "            'scaler_ablation': scaler_ablation.state_dict(),\n",
        "            'loss_hist': loss_hist,\n",
        "            'loss_abl_hist': loss_abl_hist,\n",
        "        }, checkpoint_path)\n",
        "        print(f\"Checkpoint saved at epoch {epoch}\")\n",
        "\n",
        "# Final Sigma Test\n",
        "triality_mean = np.mean(loss_hist)\n",
        "abl_mean = np.mean(loss_abl_hist)\n",
        "std = np.std(loss_hist + loss_abl_hist)\n",
        "sigma = (abl_mean - triality_mean) / std if std > 0 else 0\n",
        "\n",
        "print(f\"Final Sigma (Triality vs Ablation): {sigma:.2f} (higher = triality advantage)\")\n",
        "\n",
        "print(\"Sim complete — epochs + sigma test done\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Epoch 0 | Triality Loss 0.935768 | Ablation Loss 0.925464\n",
            "Epoch 500 | Triality Loss 0.768269 | Ablation Loss 0.758495\n",
            "Checkpoint saved at epoch 999\n",
            "Epoch 1000 | Triality Loss 0.708677 | Ablation Loss 0.698943\n",
            "Epoch 1500 | Triality Loss 0.652623 | Ablation Loss 0.646373\n",
            "Checkpoint saved at epoch 1999\n",
            "Epoch 2000 | Triality Loss 0.599045 | Ablation Loss 0.589276\n",
            "Epoch 2500 | Triality Loss 0.552379 | Ablation Loss 0.523412\n",
            "Checkpoint saved at epoch 2999\n",
            "Epoch 3000 | Triality Loss 0.502364 | Ablation Loss 0.454617\n",
            "Epoch 3500 | Triality Loss 0.451034 | Ablation Loss 0.404339\n",
            "Checkpoint saved at epoch 3999\n",
            "Epoch 4000 | Triality Loss 0.401296 | Ablation Loss 0.368810\n",
            "Epoch 4500 | Triality Loss 0.367977 | Ablation Loss 0.333781\n",
            "Checkpoint saved at epoch 4999\n",
            "Final Sigma (Triality vs Ablation): -0.15 (higher = triality advantage)\n",
            "Sim complete — epochs + sigma test done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d8e14cd"
      },
      "source": [
        "## Training Time Estimate\n",
        "\n",
        "The modified training loop executed successfully for all 5000 epochs.\n",
        "\n",
        "Given the complexity of the models (E8RoboticsFusion with 32 layers of MultiheadAttention) and the `seq_len` of 1024, `batch_size` of 64, and 5000 epochs, training on a typical Colab GPU (like a Tesla T4 or A100) would generally take a significant amount of time. Based on the verbose output and the number of epochs, an estimated total training time would be in the range of **20 to 40 minutes** for 5000 epochs, depending on the specific GPU assigned by Colab and its current load. The checkpointing every 1000 epochs, as observed in the logs, also indicates progress throughout this duration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e263a51"
      },
      "source": [
        "## Run the modified code\n",
        "\n",
        "### Subtask:\n",
        "Execute the code after applying the fix to observe if the `RuntimeError` is resolved.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93f0879d"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code cell `7309c1b6` contains the modified training loop with `retain_graph=True` added to both backward calls. Executing this cell will run the training and allow us to observe if the `RuntimeError` has been resolved.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23e581f8",
        "outputId": "4d87ca17-d5aa-493a-8623-ff2c54a2f94d"
      },
      "source": [
        "# Second cell: The sim code (optimized — 5000 epochs, larger batch, single backward)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.amp\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "import numpy as np\n",
        "from contextlib import nullcontext\n",
        "import math\n",
        "import os\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# CONFIG – optimized for speed + stability\n",
        "triality = 3\n",
        "dim = 384\n",
        "latent_dim = 8\n",
        "seq_len = 1024  # time steps (robot actions/frames)\n",
        "batch_size = 64\n",
        "epochs = 5000  # reduced for fast run (sigma trend visible early)\n",
        "lr = 5e-5\n",
        "use_amp = True\n",
        "use_checkpoint = True\n",
        "\n",
        "checkpoint_dir = \"./checkpoints\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "checkpoint_path = os.path.join(checkpoint_dir, \"proprio_vision_checkpoint.pth\")\n",
        "\n",
        "# Synthetic robotics proprio + vision fusion proxy (proprioceptive states + vision features + noise/occlusion)\n",
        "features_proprio = 128  # joint angles/torques\n",
        "features_vision = 128   # camera features\n",
        "\n",
        "robot_data = []\n",
        "for b in range(batch_size):\n",
        "    t = torch.linspace(0, 10*math.pi, seq_len, device=device)\n",
        "\n",
        "    proprio = torch.sin(t.unsqueeze(-1) * torch.arange(features_proprio, device=device)) * 0.5  # internal state\n",
        "    vision = torch.cos(t.unsqueeze(-1) * torch.arange(features_vision, device=device) * 1.2) * 0.4   # visual motion\n",
        "\n",
        "    frame = torch.cat([proprio, vision], dim=-1)\n",
        "    frame += torch.randn_like(frame) * 0.1  # noise\n",
        "    robot_data.append(frame)\n",
        "\n",
        "robot_data = torch.stack(robot_data).to(device)\n",
        "\n",
        "# Project to shared dim\n",
        "proj = nn.Linear(features_proprio + features_vision, dim).to(device)\n",
        "clean_data = proj(robot_data)\n",
        "\n",
        "# High masking (70–90% — sensor dropout/occlusion proxy)\n",
        "missing_rate = torch.linspace(0.7, 0.9, batch_size, device=device).view(batch_size, 1, 1)\n",
        "mask = torch.rand_like(clean_data) < missing_rate\n",
        "real_data = clean_data.clone()\n",
        "real_data[mask] = 0\n",
        "\n",
        "target = clean_data\n",
        "\n",
        "# E8 roots – precompute\n",
        "def get_e8_roots():\n",
        "    roots = []\n",
        "    for i in range(8):\n",
        "        for j in range(i+1, 8):\n",
        "            for signs in [(1,1), (1,-1), (-1,1), (-1,-1)]:\n",
        "                v = torch.zeros(8)\n",
        "                v[i] = signs[0]; v[j] = signs[1]\n",
        "                roots.append(v); roots.append(-v)\n",
        "    for signs in range(1 << 8):\n",
        "        v = torch.tensor([(1 if (signs & (1<<k)) else -1) for k in range(8)], dtype=torch.float32) * 0.5\n",
        "        if bin(signs).count('1') % 2 == 0:\n",
        "            roots.append(v); roots.append(-v)\n",
        "    roots = torch.stack(roots[:240])\n",
        "    return roots / roots.norm(dim=-1, keepdim=True)\n",
        "\n",
        "e8_roots = get_e8_roots().to(device)\n",
        "\n",
        "# Triality Cycle Block (detached pump scalar)\n",
        "class RoboticsCycleBlock(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(latent_dim, dim // triality, bias=False)\n",
        "        self.register_buffer('roots', e8_roots)\n",
        "\n",
        "    def forward(self, x, step):\n",
        "        pos_emb = self.roots[torch.arange(x.shape[1], device=device) % 240]\n",
        "        low_dim = self.proj(pos_emb)\n",
        "        emb = low_dim.repeat(1, triality)\n",
        "        with torch.no_grad():\n",
        "            pump_scalar = 0.8 * math.sin(step * 0.006 * 2 * math.pi)\n",
        "        pump = torch.full((1, x.shape[1], 1), pump_scalar, device=device)\n",
        "        emb_broadcast = emb.unsqueeze(0)\n",
        "        x_rot1 = x * (emb_broadcast.cos() + pump)\n",
        "        x_rot2 = torch.roll(x_rot1, shifts=1, dims=1) * emb_broadcast.sin()\n",
        "        x_rot3 = torch.roll(x_rot2, shifts=1, dims=1) * emb_broadcast.cos()\n",
        "        fused = (x_rot1 + x_rot2 + x_rot3) / triality\n",
        "        return fused\n",
        "\n",
        "# Dummy cycle for ablation\n",
        "class DummyCycle(nn.Module):\n",
        "    def forward(self, x, step=None):\n",
        "        return x\n",
        "\n",
        "# Model with ablation support\n",
        "class E8RoboticsFusion(nn.Module):\n",
        "    def __init__(self, depth=32, use_triality=True):\n",
        "        super().__init__()\n",
        "        self.use_triality = use_triality\n",
        "        self.cycle = RoboticsCycleBlock() if use_triality else DummyCycle()\n",
        "        self.layers = nn.ModuleList([nn.MultiheadAttention(dim, triality if use_triality else 8, batch_first=True) for _ in range(depth)])\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.head = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x, step):\n",
        "        x = self.cycle(x, step)\n",
        "        for layer in self.layers:\n",
        "            if use_checkpoint:\n",
        "                attn, _ = checkpoint(layer, x, x, x, use_reentrant=False)\n",
        "            else:\n",
        "                attn, _ = layer(x, x, x)\n",
        "            x = x + attn\n",
        "            x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "# Models\n",
        "model = E8RoboticsFusion(use_triality=True).to(device)\n",
        "model_ablation = E8RoboticsFusion(use_triality=False).to(device)\n",
        "\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "scaler = torch.amp.GradScaler('cuda') if use_amp else nullcontext()\n",
        "\n",
        "opt_ablation = torch.optim.AdamW(model_ablation.parameters(), lr=lr)\n",
        "scaler_ablation = torch.amp.GradScaler('cuda') if use_amp else nullcontext()\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "loss_hist = []\n",
        "loss_abl_hist = []\n",
        "\n",
        "start_epoch = 0\n",
        "\n",
        "# Load checkpoint if exists (resume on disconnect)\n",
        "if os.path.exists(checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['model'])\n",
        "    model_ablation.load_state_dict(checkpoint['model_ablation'])\n",
        "    opt.load_state_dict(checkpoint['opt'])\n",
        "    opt_ablation.load_state_dict(checkpoint['opt_ablation'])\n",
        "    scaler.load_state_dict(checkpoint['scaler'])\n",
        "    scaler_ablation.load_state_dict(checkpoint['scaler_ablation'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    loss_hist = checkpoint['loss_hist']\n",
        "    loss_abl_hist = checkpoint['loss_abl_hist']\n",
        "    print(f\"Resumed from epoch {start_epoch}\")\n",
        "\n",
        "for epoch in range(start_epoch, epochs):\n",
        "    opt.zero_grad(set_to_none=True)\n",
        "    opt_ablation.zero_grad(set_to_none=True)\n",
        "\n",
        "    with torch.amp.autocast(device_type='cuda', dtype=torch.float16) if use_amp else nullcontext():\n",
        "        recon = model(real_data, epoch)\n",
        "        loss = loss_fn(recon, target)\n",
        "\n",
        "        recon_abl = model_ablation(real_data, epoch)\n",
        "        loss_abl = loss_fn(recon_abl, target)\n",
        "\n",
        "    scaler.scale(loss).backward(retain_graph=True) if use_amp else loss.backward(retain_graph=True)\n",
        "    scaler.unscale_(opt) if use_amp else None\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1e6)\n",
        "    scaler.step(opt) if use_amp else opt.step()\n",
        "    scaler.update() if use_amp else None\n",
        "\n",
        "    scaler_ablation.scale(loss_abl).backward(retain_graph=True) if use_amp else loss_abl.backward(retain_graph=True)\n",
        "    scaler_ablation.unscale_(opt_ablation) if use_amp else None\n",
        "    torch.nn.utils.clip_grad_norm_(model_ablation.parameters(), 1e6)\n",
        "    scaler_ablation.step(opt_ablation) if use_amp else opt_ablation.step()\n",
        "    scaler_ablation.update() if use_amp else None\n",
        "\n",
        "    loss_hist.append(loss.item())\n",
        "    loss_abl_hist.append(loss_abl.item())\n",
        "\n",
        "    if epoch % 500 == 0:\n",
        "        print(f\"Epoch {epoch} | Triality Loss {loss.item():.6f} | Ablation Loss {loss_abl.item():.6f}\")\n",
        "\n",
        "    # Checkpoint every 1000 epochs\n",
        "    if (epoch + 1) % 1000 == 0:\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model': model.state_dict(),\n",
        "            'model_ablation': model_ablation.state_dict(),\n",
        "            'opt': opt.state_dict(),\n",
        "            'opt_ablation': opt_ablation.state_dict(),\n",
        "            'scaler': scaler.state_dict(),\n",
        "            'scaler_ablation': scaler_ablation.state_dict(),\n",
        "            'loss_hist': loss_hist,\n",
        "            'loss_abl_hist': loss_abl_hist,\n",
        "        }, checkpoint_path)\n",
        "        print(f\"Checkpoint saved at epoch {epoch}\")\n",
        "\n",
        "# Final Sigma Test\n",
        "triality_mean = np.mean(loss_hist)\n",
        "abl_mean = np.mean(loss_abl_hist)\n",
        "std = np.std(loss_hist + loss_abl_hist)\n",
        "sigma = (abl_mean - triality_mean) / std if std > 0 else 0\n",
        "\n",
        "print(f\"Final Sigma (Triality vs Ablation): {sigma:.2f} (higher = triality advantage)\")\n",
        "\n",
        "print(\"Sim complete — epochs + sigma test done\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Resumed from epoch 5000\n",
            "Final Sigma (Triality vs Ablation): -0.15 (higher = triality advantage)\n",
            "Sim complete — epochs + sigma test done\n"
          ]
        }
      ]
    }
  ]
}