{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "H100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "is_ZtJn4nWHR"
      },
      "outputs": [],
      "source": [
        "# First cell: Keep-alive + installs (run once — prevents disconnects)\n",
        "from IPython.display import display, Javascript\n",
        "display(Javascript('''\n",
        "function ClickConnect(){\n",
        "  console.log(\"Keeping alive\");\n",
        "  document.querySelector(\"colab-connect-button\")?.click()\n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "'''))\n",
        "print(\"Keep-alive activated — no disconnect curse\")\n",
        "\n",
        "!pip install torch matplotlib numpy\n",
        "\n",
        "# Second cell: The sim code (optimized — 3000 epochs, larger batch, single backward)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.amp\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "import numpy as np\n",
        "from contextlib import nullcontext\n",
        "import math\n",
        "import os\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# CONFIG – optimized for speed + stability\n",
        "triality = 3\n",
        "dim = 384\n",
        "latent_dim = 8\n",
        "seq_len = 1024  # \"qubit measurement\" steps\n",
        "batch_size = 64\n",
        "epochs = 3000  # reduced for fast run (sigma trend visible early)\n",
        "lr = 5e-5\n",
        "use_amp = True\n",
        "use_checkpoint = True\n",
        "\n",
        "checkpoint_dir = \"./checkpoints\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "checkpoint_path = os.path.join(checkpoint_dir, \"quantum_state_coherence_checkpoint.pth\")\n",
        "\n",
        "# Synthetic multi-qubit quantum state coherence proxy (correlated features + decoherence noise/masking)\n",
        "qubits = 4  # 4-qubit proxy (extendable)\n",
        "features_per_qubit = 64\n",
        "\n",
        "quantum_states = []\n",
        "for b in range(batch_size):\n",
        "    t = torch.linspace(0, 10*math.pi, seq_len, device=device)\n",
        "    # Correlated base (multi-qubit entangled proxy)\n",
        "    base = torch.sin(t.unsqueeze(-1) * torch.arange(features_per_qubit, device=device)) * 0.5\n",
        "    state = base.unsqueeze(-1).repeat(1, 1, qubits)  # shared across qubits\n",
        "    # Entanglement correlations (simple proxy)\n",
        "    for q in range(1, qubits):\n",
        "        state[..., q] = state[..., q] + state[..., 0] * 0.3  # correlated\n",
        "    state += torch.randn_like(state) * 0.15  # decoherence noise\n",
        "    quantum_states.append(state)\n",
        "\n",
        "quantum_states = torch.stack(quantum_states).to(device)  # (batch, seq_len, features, qubits)\n",
        "quantum_states = quantum_states.view(batch_size, seq_len, -1)  # flatten qubits\n",
        "\n",
        "# Project to dim\n",
        "proj = nn.Linear(features_per_qubit * qubits, dim).to(device)\n",
        "clean_data = proj(quantum_states)\n",
        "\n",
        "# High masking (70–90% — decoherence/dropout proxy)\n",
        "missing_rate = torch.linspace(0.7, 0.9, batch_size, device=device).view(batch_size, 1, 1)\n",
        "mask = torch.rand_like(clean_data) < missing_rate\n",
        "real_data = clean_data.clone()\n",
        "real_data[mask] = 0\n",
        "\n",
        "target = clean_data\n",
        "\n",
        "# E8 roots – precompute\n",
        "def get_e8_roots():\n",
        "    roots = []\n",
        "    for i in range(8):\n",
        "        for j in range(i+1, 8):\n",
        "            for signs in [(1,1), (1,-1), (-1,1), (-1,-1)]:\n",
        "                v = torch.zeros(8)\n",
        "                v[i] = signs[0]; v[j] = signs[1]\n",
        "                roots.append(v); roots.append(-v)\n",
        "    for signs in range(1 << 8):\n",
        "        v = torch.tensor([(1 if (signs & (1<<k)) else -1) for k in range(8)], dtype=torch.float32) * 0.5\n",
        "        if bin(signs).count('1') % 2 == 0:\n",
        "            roots.append(v); roots.append(-v)\n",
        "    roots = torch.stack(roots[:240])\n",
        "    return roots / roots.norm(dim=-1, keepdim=True)\n",
        "\n",
        "e8_roots = get_e8_roots().to(device)\n",
        "\n",
        "# Triality Cycle Block (detached pump scalar)\n",
        "class QuantumStateCycleBlock(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(latent_dim, dim // triality, bias=False)\n",
        "        self.register_buffer('roots', e8_roots)\n",
        "\n",
        "    def forward(self, x, step):\n",
        "        pos_emb = self.roots[torch.arange(x.shape[1], device=device) % 240]\n",
        "        low_dim = self.proj(pos_emb)\n",
        "        emb = low_dim.repeat(1, triality)\n",
        "        with torch.no_grad():\n",
        "            pump_scalar = 0.8 * math.sin(step * 0.006 * 2 * math.pi)\n",
        "        pump = torch.full((1, x.shape[1], 1), pump_scalar, device=device)\n",
        "        emb_broadcast = emb.unsqueeze(0)\n",
        "        x_rot1 = x * (emb_broadcast.cos() + pump)\n",
        "        x_rot2 = torch.roll(x_rot1, shifts=1, dims=1) * emb_broadcast.sin()\n",
        "        x_rot3 = torch.roll(x_rot2, shifts=1, dims=1) * emb_broadcast.cos()\n",
        "        fused = (x_rot1 + x_rot2 + x_rot3) / triality\n",
        "        return fused\n",
        "\n",
        "# Dummy cycle for ablation\n",
        "class DummyCycle(nn.Module):\n",
        "    def forward(self, x, step=None):\n",
        "        return x\n",
        "\n",
        "# Model with ablation support\n",
        "class E8QuantumStateFusion(nn.Module):\n",
        "    def __init__(self, depth=32, use_triality=True):\n",
        "        super().__init__()\n",
        "        self.use_triality = use_triality\n",
        "        self.cycle = QuantumStateCycleBlock() if use_triality else DummyCycle()\n",
        "        self.layers = nn.ModuleList([nn.MultiheadAttention(dim, triality if use_triality else 8, batch_first=True) for _ in range(depth)])\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.head = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x, step):\n",
        "        x = self.cycle(x, step)\n",
        "        for layer in self.layers:\n",
        "            if use_checkpoint:\n",
        "                attn, _ = checkpoint(layer, x, x, x, use_reentrant=False)\n",
        "            else:\n",
        "                attn, _ = layer(x, x, x)\n",
        "            x = x + attn\n",
        "            x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "# Models\n",
        "model = E8QuantumStateFusion(use_triality=True).to(device)\n",
        "model_ablation = E8QuantumStateFusion(use_triality=False).to(device)\n",
        "\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "scaler = torch.amp.GradScaler('cuda') if use_amp else nullcontext()\n",
        "\n",
        "opt_ablation = torch.optim.AdamW(model_ablation.parameters(), lr=lr)\n",
        "scaler_ablation = torch.amp.GradScaler('cuda') if use_amp else nullcontext()\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "loss_hist = []\n",
        "loss_abl_hist = []\n",
        "\n",
        "start_epoch = 0\n",
        "\n",
        "# Load checkpoint if exists (resume on disconnect)\n",
        "if os.path.exists(checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['model'])\n",
        "    model_ablation.load_state_dict(checkpoint['model_ablation'])\n",
        "    opt.load_state_dict(checkpoint['opt'])\n",
        "    opt_ablation.load_state_dict(checkpoint['opt_ablation'])\n",
        "    scaler.load_state_dict(checkpoint['scaler'])\n",
        "    scaler_ablation.load_state_dict(checkpoint['scaler_ablation'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    loss_hist = checkpoint['loss_hist']\n",
        "    loss_abl_hist = checkpoint['loss_abl_hist']\n",
        "    print(f\"Resumed from epoch {start_epoch}\")\n",
        "\n",
        "for epoch in range(start_epoch, epochs):\n",
        "    opt.zero_grad(set_to_none=True)\n",
        "    opt_ablation.zero_grad(set_to_none=True)\n",
        "\n",
        "    with torch.amp.autocast(device_type='cuda', dtype=torch.float16) if use_amp else nullcontext():\n",
        "        recon = model(real_data, epoch)\n",
        "        loss = loss_fn(recon, target)\n",
        "\n",
        "        recon_abl = model_ablation(real_data, epoch)\n",
        "        loss_abl = loss_fn(recon_abl, target)\n",
        "\n",
        "    scaler.scale(loss).backward() if use_amp else loss.backward()\n",
        "    scaler.unscale_(opt) if use_amp else None\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1e6)\n",
        "    scaler.step(opt) if use_amp else opt.step()\n",
        "    scaler.update() if use_amp else None\n",
        "\n",
        "    scaler_ablation.scale(loss_abl).backward() if use_amp else loss_abl.backward()\n",
        "    scaler_ablation.unscale_(opt_ablation) if use_amp else None\n",
        "    torch.nn.utils.clip_grad_norm_(model_ablation.parameters(), 1e6)\n",
        "    scaler_ablation.step(opt_ablation) if use_amp else opt_ablation.step()\n",
        "    scaler_ablation.update() if use_amp else None\n",
        "\n",
        "    loss_hist.append(loss.item())\n",
        "    loss_abl_hist.append(loss_abl.item())\n",
        "\n",
        "    if epoch % 500 == 0:\n",
        "        print(f\"Epoch {epoch} | Triality Loss {loss.item():.6f} | Ablation Loss {loss_abl.item():.6f}\")\n",
        "\n",
        "    # Checkpoint every 1000 epochs\n",
        "    if (epoch + 1) % 1000 == 0:\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model': model.state_dict(),\n",
        "            'model_ablation': model_ablation.state_dict(),\n",
        "            'opt': opt.state_dict(),\n",
        "            'opt_ablation': opt_ablation.state_dict(),\n",
        "            'scaler': scaler.state_dict(),\n",
        "            'scaler_ablation': scaler_ablation.state_dict(),\n",
        "            'loss_hist': loss_hist,\n",
        "            'loss_abl_hist': loss_abl_hist,\n",
        "        }, checkpoint_path)\n",
        "        print(f\"Checkpoint saved at epoch {epoch}\")\n",
        "\n",
        "# Final Sigma Test\n",
        "triality_mean = np.mean(loss_hist)\n",
        "abl_mean = np.mean(loss_abl_hist)\n",
        "std = np.std(loss_hist + loss_abl_hist)\n",
        "sigma = (abl_mean - triality_mean) / std if std > 0 else 0\n",
        "\n",
        "print(f\"Final Sigma (Triality vs Ablation): {sigma:.2f} (higher = triality advantage)\")\n",
        "\n",
        "print(\"Sim complete — epochs + sigma test done\")"
      ]
    }
  ]
}