{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.amp\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "import numpy as np\n",
        "from contextlib import nullcontext\n",
        "import math\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# CONFIG – optimized for Colab A100 high-RAM (fast epochs)\n",
        "triality = 3\n",
        "dim = 384\n",
        "latent_dim = 8\n",
        "seq_len = 2048  # audio samples proxy\n",
        "batch_size = 32\n",
        "epochs = 20000\n",
        "lr = 5e-5\n",
        "use_amp = True\n",
        "use_checkpoint = True\n",
        "\n",
        "# Synthetic LibriSpeech audio proxy (real-like speech waveforms + noise/occlusion)\n",
        "audio = []\n",
        "for b in range(batch_size):\n",
        "    t = torch.linspace(0, 10*math.pi, seq_len, device=device)\n",
        "    wave = torch.sin(t * (20 + b % 10)) * 0.5  # \"speech-like\" frequency variation\n",
        "    wave += torch.randn_like(wave) * 0.2  # noise\n",
        "    wave = torch.clamp(wave, -1, 1)\n",
        "    audio.append(wave.unsqueeze(-1))  # add feature dim\n",
        "\n",
        "audio = torch.cat(audio, dim=0).to(device)  # (batch*seq_len, 1) -> reshape\n",
        "audio = audio.view(batch_size, seq_len, -1)\n",
        "\n",
        "# Project to dim\n",
        "proj = nn.Linear(1, dim).to(device)\n",
        "with torch.no_grad(): # Detach the input features from the graph for both models\n",
        "    clean_data = proj(audio)\n",
        "\n",
        "# Apply masking (40–70% missing samples — noise/dropouts)\n",
        "missing_rate = torch.linspace(0.4, 0.7, batch_size, device=device).view(batch_size, 1, 1)\n",
        "mask = torch.rand_like(clean_data) < missing_rate\n",
        "real_data = clean_data.clone()\n",
        "real_data[mask] = 0\n",
        "\n",
        "target = clean_data\n",
        "\n",
        "# E8 roots – precompute\n",
        "def get_e8_roots():\n",
        "    roots = []\n",
        "    for i in range(8):\n",
        "        for j in range(i+1, 8):\n",
        "            for signs in [(1,1), (1,-1), (-1,1), (-1,-1)]:\n",
        "                v = torch.zeros(8)\n",
        "                v[i] = signs[0]; v[j] = signs[1]\n",
        "                roots.append(v); roots.append(-v)\n",
        "    for signs in range(1 << 8):\n",
        "        v = torch.tensor([(1 if (signs & (1<<k)) else -1) for k in range(8)], dtype=torch.float32) * 0.5\n",
        "        if bin(signs).count('1') % 2 == 0:\n",
        "            roots.append(v); roots.append(-v)\n",
        "    roots = torch.stack(roots[:240])\n",
        "    return roots / roots.norm(dim=-1, keepdim=True)\n",
        "\n",
        "e8_roots = get_e8_roots().to(device)\n",
        "\n",
        "# Triality Cycle Block (detached pump scalar)\n",
        "class AudioCycleBlock(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(latent_dim, dim // triality, bias=False)\n",
        "        self.register_buffer('roots', e8_roots)\n",
        "\n",
        "    def forward(self, x, step):\n",
        "        pos_emb = self.roots[torch.arange(x.shape[1], device=device) % 240]\n",
        "        low_dim = self.proj(pos_emb)\n",
        "        emb = low_dim.repeat(1, triality)\n",
        "        with torch.no_grad():\n",
        "            pump_scalar = 0.8 * math.sin(step * 0.006 * 2 * math.pi)\n",
        "        pump = torch.full((1, x.shape[1], 1), pump_scalar, device=device)\n",
        "        emb_broadcast = emb.unsqueeze(0)\n",
        "        x_rot1 = x * (emb_broadcast.cos() + pump)\n",
        "        x_rot2 = torch.roll(x_rot1, shifts=1, dims=1) * emb_broadcast.sin()\n",
        "        x_rot3 = torch.roll(x_rot2, shifts=1, dims=1) * emb_broadcast.cos()\n",
        "        fused = (x_rot1 + x_rot2 + x_rot3) / triality\n",
        "        return fused\n",
        "\n",
        "# Dummy cycle for ablation\n",
        "class DummyCycle(nn.Module):\n",
        "    def forward(self, x, step=None):\n",
        "        return x\n",
        "\n",
        "# Model with ablation support\n",
        "class E8AudioFusion(nn.Module):\n",
        "    def __init__(self, depth=32, use_triality=True):\n",
        "        super().__init__()\n",
        "        self.use_triality = use_triality\n",
        "        self.cycle = AudioCycleBlock() if use_triality else DummyCycle()\n",
        "        self.layers = nn.ModuleList([nn.MultiheadAttention(dim, triality if use_triality else 8, batch_first=True) for _ in range(depth)])\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.head = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x, step):\n",
        "        x = self.cycle(x, step)\n",
        "        for layer in self.layers:\n",
        "            if use_checkpoint:\n",
        "                attn, _ = checkpoint(layer, x, x, x, use_reentrant=False)\n",
        "            else:\n",
        "                attn, _ = layer(x, x, x)\n",
        "            x = x + self.norm(attn)\n",
        "        return x\n",
        "\n",
        "# Models\n",
        "model = E8AudioFusion(use_triality=True).to(device)\n",
        "model_ablation = E8AudioFusion(use_triality=False).to(device)\n",
        "\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "scaler = torch.amp.GradScaler('cuda') if use_amp else nullcontext()\n",
        "\n",
        "opt_ablation = torch.optim.AdamW(model_ablation.parameters(), lr=lr)\n",
        "scaler_ablation = torch.amp.GradScaler('cuda') if use_amp else nullcontext()\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "loss_hist = []\n",
        "loss_abl_hist = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # --- Training for 'model' ---\n",
        "    opt.zero_grad(set_to_none=True)\n",
        "    with torch.amp.autocast(device_type='cuda', dtype=torch.float16) if use_amp else nullcontext():\n",
        "        recon = model(real_data, epoch)\n",
        "        loss = loss_fn(recon, target)\n",
        "\n",
        "    scaler.scale(loss).backward() if use_amp else loss.backward()\n",
        "    scaler.unscale_(opt) if use_amp else None\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1e6)\n",
        "    scaler.step(opt) if use_amp else opt.step()\n",
        "    scaler.update() if use_amp else None\n",
        "\n",
        "    # --- Training for 'model_ablation' ---\n",
        "    opt_ablation.zero_grad(set_to_none=True)\n",
        "    with torch.amp.autocast(device_type='cuda', dtype=torch.float16) if use_amp else nullcontext():\n",
        "        recon_abl = model_ablation(real_data, epoch)\n",
        "        loss_abl = loss_fn(recon_abl, target)\n",
        "\n",
        "    scaler_ablation.scale(loss_abl).backward() if use_amp else loss_abl.backward()\n",
        "    scaler_ablation.unscale_(opt_ablation) if use_amp else None\n",
        "    torch.nn.utils.clip_grad_norm_(model_ablation.parameters(), 1e6)\n",
        "    scaler_ablation.step(opt_ablation) if use_amp else opt_ablation.step()\n",
        "    scaler_ablation.update() if use_amp else None\n",
        "\n",
        "    loss_hist.append(loss.item())\n",
        "    loss_abl_hist.append(loss_abl.item())\n",
        "\n",
        "    if epoch % 500 == 0:\n",
        "        print(f\"Epoch {epoch} | Triality Loss {loss.item():.6f} | Ablation Loss {loss_abl.item():.6f}\")\n",
        "\n",
        "# Final Sigma Test\n",
        "triality_mean = np.mean(loss_hist)\n",
        "abl_mean = np.mean(loss_abl_hist)\n",
        "std = np.std(loss_hist + loss_abl_hist)\n",
        "sigma = (abl_mean - triality_mean) / std if std > 0 else 0\n",
        "\n",
        "print(f\"Final Sigma (Triality vs Ablation): {sigma:.2f} (higher = triality advantage)\")\n",
        "\n",
        "print(\"Sim complete — epochs + sigma test done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiynhCgx9fRj",
        "outputId": "fd9a0227-4e76-437f-f875-df20603df478"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Epoch 0 | Triality Loss 30.929083 | Ablation Loss 33.248138\n",
            "Epoch 500 | Triality Loss 0.057787 | Ablation Loss 0.122760\n",
            "Epoch 1000 | Triality Loss 0.051712 | Ablation Loss 0.112118\n",
            "Epoch 1500 | Triality Loss 0.029844 | Ablation Loss 0.102177\n",
            "Epoch 2000 | Triality Loss 0.024745 | Ablation Loss 0.096813\n",
            "Epoch 2500 | Triality Loss 0.020120 | Ablation Loss 0.091855\n",
            "Epoch 3000 | Triality Loss 0.018723 | Ablation Loss 0.085701\n",
            "Epoch 3500 | Triality Loss 0.018299 | Ablation Loss 0.082985\n",
            "Epoch 4000 | Triality Loss 0.017631 | Ablation Loss 0.075469\n",
            "Epoch 4500 | Triality Loss 0.018833 | Ablation Loss 0.071593\n",
            "Epoch 5000 | Triality Loss 0.017343 | Ablation Loss 0.067354\n",
            "Epoch 5500 | Triality Loss 0.017486 | Ablation Loss 0.061201\n",
            "Epoch 6000 | Triality Loss 0.017270 | Ablation Loss 0.056687\n",
            "Epoch 6500 | Triality Loss 0.018546 | Ablation Loss 0.051620\n",
            "Epoch 7000 | Triality Loss 0.015401 | Ablation Loss 0.049099\n",
            "Epoch 7500 | Triality Loss 0.015076 | Ablation Loss 0.043938\n",
            "Epoch 8000 | Triality Loss 0.015114 | Ablation Loss 0.039442\n",
            "Epoch 8500 | Triality Loss 0.014177 | Ablation Loss 0.036923\n",
            "Epoch 9000 | Triality Loss 0.013667 | Ablation Loss 0.032907\n",
            "Epoch 9500 | Triality Loss 0.013656 | Ablation Loss 0.030967\n"
          ]
        }
      ]
    }
  ]
}