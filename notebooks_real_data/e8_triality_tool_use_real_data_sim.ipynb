{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "H100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# First cell: Keep-alive + installs (run once — prevents disconnects)\n",
        "from IPython.display import display, Javascript\n",
        "display(Javascript('''\n",
        "function ClickConnect(){\n",
        "  console.log(\"Keeping alive\");\n",
        "  document.querySelector(\"colab-connect-button\")?.click()\n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "'''))\n",
        "print(\"Keep-alive activated — no disconnect curse\")\n",
        "\n",
        "!pip install torch matplotlib numpy\n",
        "\n",
        "# Second cell: The sim code (optimized — 5000 epochs, high masking)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.amp\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "import numpy as np\n",
        "from contextlib import nullcontext\n",
        "import math\n",
        "import os\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# CONFIG – optimized for speed + stability\n",
        "triality = 3\n",
        "dim = 384\n",
        "latent_dim = 8\n",
        "seq_len = 512 # Re-reduced to 512 to avoid OutOfMemoryError\n",
        "batch_size = 32\n",
        "epochs = 5000  # reduced for fast run (sigma trend visible early)\n",
        "lr = 5e-5\n",
        "use_amp = True\n",
        "use_checkpoint = False # Temporarily disabled to debug RuntimeError\n",
        "\n",
        "checkpoint_dir = \"./checkpoints\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "checkpoint_path = os.path.join(checkpoint_dir, \"tool_use_checkpoint.pth\")\n",
        "\n",
        "# Synthetic tool use proxy (real-like agent tool calls + drift/noise)\n",
        "features = 64\n",
        "\n",
        "tool_sequences = []\n",
        "for b in range(batch_size):\n",
        "    t = torch.linspace(0, 10*math.pi, seq_len, device=device)\n",
        "    seq = torch.sin(t.unsqueeze(-1) * torch.arange(features, device=device)) * 0.5\n",
        "    seq += torch.randn_like(seq) * 0.3  # higher noise\n",
        "    tool_sequences.append(seq)\n",
        "\n",
        "tool_sequences = torch.stack(tool_sequences).to(device)\n",
        "\n",
        "proj = nn.Linear(features, dim).to(device)\n",
        "clean_data = proj(tool_sequences).detach() # Detach clean_data here to prevent graph conflicts\n",
        "\n",
        "# High masking (70–90% — partial tool calls proxy)\n",
        "missing_rate = torch.linspace(0.7, 0.9, batch_size, device=device).view(batch_size, 1, 1)\n",
        "mask = torch.rand_like(clean_data) < missing_rate\n",
        "real_data = clean_data.clone()\n",
        "real_data[mask] = 0\n",
        "\n",
        "target = clean_data # No need to detach again, as clean_data is already detached\n",
        "\n",
        "# E8 roots – precompute\n",
        "def get_e8_roots():\n",
        "    roots = []\n",
        "    for i in range(8):\n",
        "        for j in range(i+1, 8):\n",
        "            for signs in [(1,1), (1,-1), (-1,1), (-1,-1)]:\n",
        "                v = torch.zeros(8)\n",
        "                v[i] = signs[0]; v[j] = signs[1]\n",
        "                roots.append(v); roots.append(-v)\n",
        "    for signs in range(1 << 8):\n",
        "        v = torch.tensor([(1 if (signs & (1<<k)) else -1) for k in range(8)], dtype=torch.float32) * 0.5\n",
        "        if bin(signs).count('1') % 2 == 0:\n",
        "            roots.append(v); roots.append(-v)\n",
        "    roots = torch.stack(roots[:240])\n",
        "    return roots / roots.norm(dim=-1, keepdim=True)\n",
        "\n",
        "e8_roots = get_e8_roots().to(device)\n",
        "\n",
        "# Triality Cycle Block (detached pump scalar)\n",
        "class ToolCycleBlock(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(latent_dim, dim // triality, bias=False)\n",
        "        self.register_buffer('roots', e8_roots)\n",
        "\n",
        "    def forward(self, x, step):\n",
        "        pos_emb = self.roots[torch.arange(x.shape[1], device=device) % 240]\n",
        "        low_dim = self.proj(pos_emb)\n",
        "        emb = low_dim.repeat(1, triality)\n",
        "        with torch.no_grad():\n",
        "            pump_scalar = 0.8 * math.sin(step * 0.006 * 2 * math.pi)\n",
        "        pump = torch.full((1, x.shape[1], 1), pump_scalar, device=device)\n",
        "        emb_broadcast = emb.unsqueeze(0)\n",
        "        x_rot1 = x * (emb_broadcast.cos() + pump)\n",
        "        x_rot2 = torch.roll(x_rot1, shifts=1, dims=1) * emb_broadcast.sin()\n",
        "        x_rot3 = torch.roll(x_rot2, shifts=1, dims=1) * emb_broadcast.cos()\n",
        "        fused = (x_rot1 + x_rot2 + x_rot3) / triality\n",
        "        return fused\n",
        "\n",
        "# Dummy cycle for ablation\n",
        "class DummyCycle(nn.Module):\n",
        "    def forward(self, x, step=None):\n",
        "        return x\n",
        "\n",
        "# Model with ablation support\n",
        "class E8ToolFusion(nn.Module):\n",
        "    def __init__(self, depth=32, use_triality=True):\n",
        "        super().__init__()\n",
        "        self.use_triality = use_triality\n",
        "        self.cycle = ToolCycleBlock() if use_triality else DummyCycle()\n",
        "        self.layers = nn.ModuleList([nn.MultiheadAttention(dim, triality if use_triality else 8, batch_first=True) for _ in range(depth)])\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.head = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x, step):\n",
        "        x = self.cycle(x, step)\n",
        "        for layer in self.layers:\n",
        "            if use_checkpoint:\n",
        "                attn, _ = checkpoint(layer, x, x, x, use_reentrant=False)\n",
        "            else:\n",
        "                attn, _ = layer(x, x, x)\n",
        "            x = x + self.norm(attn)\n",
        "        return x\n",
        "\n",
        "# Models\n",
        "model = E8ToolFusion(use_triality=True).to(device)\n",
        "model_ablation = E8ToolFusion(use_triality=False).to(device)\n",
        "\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "scaler = torch.amp.GradScaler('cuda') if use_amp else nullcontext()\n",
        "\n",
        "opt_ablation = torch.optim.AdamW(model_ablation.parameters(), lr=lr)\n",
        "scaler_ablation = torch.amp.GradScaler('cuda') if use_amp else nullcontext()\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "loss_hist = []\n",
        "loss_abl_hist = []\n",
        "\n",
        "start_epoch = 0\n",
        "\n",
        "# Load checkpoint if exists (resume on disconnect)\n",
        "if os.path.exists(checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['model'])\n",
        "    model_ablation.load_state_dict(checkpoint['model_ablation'])\n",
        "    opt.load_state_dict(checkpoint['opt'])\n",
        "    opt_ablation.load_state_dict(checkpoint['opt_ablation'])\n",
        "    scaler.load_state_dict(checkpoint['scaler'])\n",
        "    scaler_ablation.load_state_dict(checkpoint['scaler_ablation'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    loss_hist = checkpoint['loss_hist']\n",
        "    loss_abl_hist = checkpoint['loss_abl_hist']\n",
        "    print(f\"Resumed from epoch {start_epoch}\")\n",
        "\n",
        "for epoch in range(start_epoch, epochs):\n",
        "    # Model 1 (Triality) update\n",
        "    opt.zero_grad(set_to_none=True)\n",
        "    with torch.amp.autocast(device_type='cuda', dtype=torch.float16) if use_amp else nullcontext():\n",
        "        recon = model(real_data.clone(), epoch)\n",
        "        loss = loss_fn(recon, target)\n",
        "    scaler.scale(loss).backward() if use_amp else loss.backward() # retain_graph=True is no longer needed here\n",
        "    scaler.unscale_(opt) if use_amp else None\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1e6)\n",
        "    scaler.step(opt) if use_amp else opt.step()\n",
        "    scaler.update() if use_amp else None\n",
        "    loss_hist.append(loss.item())\n",
        "\n",
        "    # Model 2 (Ablation) update\n",
        "    opt_ablation.zero_grad(set_to_none=True)\n",
        "    with torch.amp.autocast(device_type='cuda', dtype=torch.float16) if use_amp else nullcontext():\n",
        "        recon_abl = model_ablation(real_data.clone(), epoch)\n",
        "        loss_abl = loss_fn(recon_abl, target)\n",
        "    scaler_ablation.scale(loss_abl).backward() if use_amp else loss_abl.backward()\n",
        "    scaler_ablation.unscale_(opt_ablation) if use_amp else None\n",
        "    torch.nn.utils.clip_grad_norm_(model_ablation.parameters(), 1e6)\n",
        "    scaler_ablation.step(opt_ablation) if use_amp else opt_ablation.step()\n",
        "    scaler_ablation.update() if use_amp else None\n",
        "    loss_abl_hist.append(loss_abl.item())\n",
        "\n",
        "    if epoch % 500 == 0:\n",
        "        print(f\"Epoch {epoch} | Triality Loss {loss.item():.6f} | Ablation Loss {loss_abl.item():.6f}\")\n",
        "\n",
        "    # Checkpoint every 1000 epochs\n",
        "    if (epoch + 1) % 1000 == 0:\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model': model.state_dict(),\n",
        "            'model_ablation': model_ablation.state_dict(),\n",
        "            'opt': opt.state_dict(),\n",
        "            'opt_ablation': opt_ablation.state_dict(),\n",
        "            'scaler': scaler.state_dict(),\n",
        "            'scaler_ablation': scaler_ablation.state_dict(),\n",
        "            'loss_hist': loss_hist,\n",
        "            'loss_abl_hist': loss_abl_hist,\n",
        "        }, checkpoint_path)\n",
        "        print(f\"Checkpoint saved at epoch {epoch}\")\n",
        "\n",
        "# Final Sigma Test\n",
        "triality_mean = np.mean(loss_hist)\n",
        "abl_mean = np.mean(loss_abl_hist)\n",
        "std = np.std(loss_hist + loss_abl_hist)\n",
        "sigma = (abl_mean - triality_mean) / std if std > 0 else 0\n",
        "\n",
        "print(f\"Final Sigma (Triality vs Ablation): {sigma:.2f} (higher = triality advantage)\")\n",
        "\n",
        "print(\"Sim complete — epochs + sigma test done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cVhYp9jiQNJ9",
        "outputId": "10b4207c-1162-4dee-a1fa-019c4bc3aa2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "function ClickConnect(){\n",
              "  console.log(\"Keeping alive\");\n",
              "  document.querySelector(\"colab-connect-button\")?.click()\n",
              "}\n",
              "setInterval(ClickConnect,60000)\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keep-alive activated — no disconnect curse\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (26.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Using device: cuda\n",
            "Epoch 0 | Triality Loss 32.025021 | Ablation Loss 32.136639\n",
            "Epoch 500 | Triality Loss 0.085470 | Ablation Loss 0.069459\n",
            "Checkpoint saved at epoch 999\n",
            "Epoch 1000 | Triality Loss 0.072806 | Ablation Loss 0.063180\n",
            "Epoch 1500 | Triality Loss 0.068898 | Ablation Loss 0.060458\n",
            "Checkpoint saved at epoch 1999\n",
            "Epoch 2000 | Triality Loss 0.067583 | Ablation Loss 0.060644\n",
            "Epoch 2500 | Triality Loss 0.066241 | Ablation Loss 0.058911\n",
            "Checkpoint saved at epoch 2999\n",
            "Epoch 3000 | Triality Loss 0.065733 | Ablation Loss 0.058620\n",
            "Epoch 3500 | Triality Loss 0.065187 | Ablation Loss 0.057234\n",
            "Checkpoint saved at epoch 3999\n",
            "Epoch 4000 | Triality Loss 0.065109 | Ablation Loss 0.056118\n",
            "Epoch 4500 | Triality Loss 0.064679 | Ablation Loss 0.055817\n",
            "Checkpoint saved at epoch 4999\n",
            "Final Sigma (Triality vs Ablation): 0.00 (higher = triality advantage)\n",
            "Sim complete — epochs + sigma test done\n"
          ]
        }
      ]
    }
  ]
}