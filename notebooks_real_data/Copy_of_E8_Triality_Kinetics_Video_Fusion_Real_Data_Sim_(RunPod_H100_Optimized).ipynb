{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "H100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJs7cI21PwIK",
        "outputId": "38d04531-2f58-485c-ac6e-7f9e51006dc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.3.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (8.3.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | Triality Loss 54.625847 | Ablation Loss 53.989761\n",
            "Epoch 500 | Triality Loss 20.551643 | Ablation Loss 13.411509\n",
            "Epoch 1000 | Triality Loss 9.946285 | Ablation Loss 6.492839\n",
            "Epoch 1500 | Triality Loss 4.784335 | Ablation Loss 3.348026\n",
            "Epoch 2000 | Triality Loss 2.653273 | Ablation Loss 1.582970\n",
            "Epoch 2500 | Triality Loss 1.770790 | Ablation Loss 0.791277\n",
            "Epoch 3000 | Triality Loss 1.373427 | Ablation Loss 0.341355\n",
            "Epoch 3500 | Triality Loss 1.104346 | Ablation Loss 0.173082\n",
            "Epoch 4000 | Triality Loss 1.405303 | Ablation Loss 0.099992\n",
            "Epoch 4500 | Triality Loss 0.529033 | Ablation Loss 0.064550\n",
            "Epoch 5000 | Triality Loss 0.439153 | Ablation Loss 0.045322\n",
            "Epoch 5500 | Triality Loss 0.400209 | Ablation Loss 0.031901\n",
            "Epoch 6000 | Triality Loss 0.311111 | Ablation Loss 0.027798\n",
            "Epoch 6500 | Triality Loss 0.285452 | Ablation Loss 0.018112\n",
            "Epoch 7000 | Triality Loss 0.265935 | Ablation Loss 0.013984\n",
            "Epoch 7500 | Triality Loss 0.207237 | Ablation Loss 0.010968\n",
            "Epoch 8000 | Triality Loss 0.131494 | Ablation Loss 0.008651\n",
            "Epoch 8500 | Triality Loss 0.123604 | Ablation Loss 0.006800\n",
            "Epoch 9000 | Triality Loss 0.140714 | Ablation Loss 0.005311\n",
            "Epoch 9500 | Triality Loss 0.111887 | Ablation Loss 0.004148\n",
            "Epoch 10000 | Triality Loss 0.073426 | Ablation Loss 0.012573\n",
            "Epoch 10500 | Triality Loss 0.045965 | Ablation Loss 0.006662\n",
            "Epoch 11000 | Triality Loss 0.032774 | Ablation Loss 0.004652\n",
            "Epoch 11500 | Triality Loss 0.032652 | Ablation Loss 0.003568\n",
            "Epoch 12000 | Triality Loss 0.023932 | Ablation Loss 0.002859\n",
            "Epoch 12500 | Triality Loss 0.019697 | Ablation Loss 0.002339\n",
            "Epoch 13000 | Triality Loss 0.018679 | Ablation Loss 0.001936\n",
            "Epoch 13500 | Triality Loss 0.012710 | Ablation Loss 0.001611\n",
            "Epoch 14000 | Triality Loss 0.011022 | Ablation Loss 0.001329\n",
            "Epoch 14500 | Triality Loss 0.007168 | Ablation Loss 0.001096\n",
            "Epoch 15000 | Triality Loss 0.009548 | Ablation Loss 0.000895\n",
            "Epoch 15500 | Triality Loss 0.009934 | Ablation Loss 0.000732\n",
            "Epoch 16000 | Triality Loss 0.005698 | Ablation Loss 0.000596\n",
            "Epoch 16500 | Triality Loss 0.004924 | Ablation Loss 0.000483\n",
            "Epoch 17000 | Triality Loss 0.018682 | Ablation Loss 0.010077\n",
            "Epoch 17500 | Triality Loss 0.006849 | Ablation Loss 0.002641\n",
            "Epoch 18000 | Triality Loss 0.002908 | Ablation Loss 0.001582\n",
            "Epoch 18500 | Triality Loss 0.001777 | Ablation Loss 0.001125\n",
            "Epoch 19000 | Triality Loss 0.000913 | Ablation Loss 0.000863\n",
            "Epoch 19500 | Triality Loss 0.000555 | Ablation Loss 0.000691\n",
            "Final Sigma (Triality vs Ablation): -0.28 (higher = triality advantage)\n",
            "Sim complete — epochs + sigma test done\n"
          ]
        }
      ],
      "source": [
        "# First cell: Set env + install (run once)\n",
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # fragmentation fix\n",
        "\n",
        "!pip install torch torchvision matplotlib numpy transformers\n",
        "\n",
        "# Second cell: The sim code (fixed — separate full training loops per epoch)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.amp\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "from contextlib import nullcontext\n",
        "import math\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# CONFIG – optimized for no OOM + H100 speed (fast epochs)\n",
        "triality = 3\n",
        "dim = 192  # reduced\n",
        "latent_dim = 8\n",
        "seq_len = 512  # reduced\n",
        "batch_size = 16  # reduced\n",
        "accum_steps = 4  # simulate larger batch\n",
        "epochs = 20000\n",
        "lr = 5e-5\n",
        "use_amp = True\n",
        "use_checkpoint = True\n",
        "\n",
        "# Synthetic code proxy (real-like Python tokens + noise/masking)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "codes = []\n",
        "for b in range(batch_size):\n",
        "    base = torch.sin(torch.linspace(0, 10*math.pi, seq_len, device=device)) * 100 + 200\n",
        "    base = base % tokenizer.vocab_size\n",
        "    code = base.long() + torch.randint(-50, 50, (seq_len,), device=device)\n",
        "    code = torch.clamp(code, 0, tokenizer.vocab_size - 1)\n",
        "    codes.append(code)\n",
        "\n",
        "codes = torch.stack(codes).to(device)\n",
        "\n",
        "embed = nn.Embedding(tokenizer.vocab_size, dim).to(device)\n",
        "clean_data = embed(codes)\n",
        "\n",
        "missing_rate = torch.linspace(0.4, 0.7, batch_size, device=device).view(batch_size, 1, 1)\n",
        "mask = torch.rand_like(clean_data) < missing_rate\n",
        "# Fix: Avoid in-place modification of real_data to maintain graph integrity\n",
        "real_data = clean_data.clone()\n",
        "real_data = torch.where(mask, torch.zeros_like(real_data), real_data)\n",
        "# Detach real_data to prevent graph issues when embed is not part of optimizers\n",
        "real_data = real_data.detach()\n",
        "\n",
        "target = codes\n",
        "\n",
        "# E8 roots – precompute\n",
        "def get_e8_roots():\n",
        "    roots = []\n",
        "    for i in range(8):\n",
        "        for j in range(i+1, 8):\n",
        "            for signs in [(1,1), (1,-1), (-1,1), (-1,-1)]:\n",
        "                v = torch.zeros(8)\n",
        "                v[i] = signs[0]; v[j] = signs[1]\n",
        "                roots.append(v); roots.append(-v)\n",
        "    for signs in range(1 << 8):\n",
        "        v = torch.tensor([(1 if (signs & (1<<k)) else -1) for k in range(8)], dtype=torch.float32) * 0.5\n",
        "        if bin(signs).count('1') % 2 == 0:\n",
        "            roots.append(v); roots.append(-v)\n",
        "    roots = torch.stack(roots[:240])\n",
        "    return roots / roots.norm(dim=-1, keepdim=True)\n",
        "\n",
        "e8_roots = get_e8_roots().to(device)\n",
        "\n",
        "# Triality Cycle Block (detached pump scalar)\n",
        "class CodeCycleBlock(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(latent_dim, dim // triality, bias=False)\n",
        "        self.register_buffer('roots', e8_roots)\n",
        "\n",
        "    def forward(self, x, step):\n",
        "        pos_emb = self.roots[torch.arange(x.shape[1], device=device) % 240]\n",
        "        low_dim = self.proj(pos_emb)\n",
        "        emb = low_dim.repeat(1, triality)\n",
        "        with torch.no_grad():\n",
        "            pump_scalar = 0.8 * math.sin(step * 0.006 * 2 * math.pi)\n",
        "        pump = torch.full((1, x.shape[1], 1), pump_scalar, device=device)\n",
        "        emb_broadcast = emb.unsqueeze(0)\n",
        "        x_rot1 = x * (emb_broadcast.cos() + pump)\n",
        "        x_rot2 = torch.roll(x_rot1, shifts=1, dims=1) * emb_broadcast.sin()\n",
        "        x_rot3 = torch.roll(x_rot2, shifts=1, dims=1) * emb_broadcast.cos()\n",
        "        fused = (x_rot1 + x_rot2 + x_rot3) / triality\n",
        "        return fused\n",
        "\n",
        "# Dummy cycle for ablation\n",
        "class DummyCycle(nn.Module):\n",
        "    def forward(self, x, step=None):\n",
        "        return x\n",
        "\n",
        "# Model with ablation support (reduced depth)\n",
        "class E8CodeFusion(nn.Module):\n",
        "    def __init__(self, depth=16, use_triality=True):  # reduced\n",
        "        super().__init__()\n",
        "        self.use_triality = use_triality\n",
        "        self.cycle = CodeCycleBlock() if use_triality else DummyCycle()\n",
        "        num_heads = triality if use_triality else 8\n",
        "        self.layers = nn.ModuleList([nn.MultiheadAttention(dim, num_heads, batch_first=True) for _ in range(depth)])\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.head = nn.Linear(dim, tokenizer.vocab_size)\n",
        "\n",
        "    def forward(self, x, step):\n",
        "        x = self.cycle(x, step)\n",
        "        for layer in self.layers:\n",
        "            if use_checkpoint:\n",
        "                # Changed use_reentrant from False to True for compatibility with retain_graph during gradient accumulation\n",
        "                attn, _ = checkpoint(layer, x, x, x, use_reentrant=True)\n",
        "            else:\n",
        "                attn, _ = layer(x, x, x)\n",
        "            x = x + self.norm(attn)\n",
        "        return self.head(x)\n",
        "\n",
        "# Models\n",
        "model = E8CodeFusion(use_triality=True).to(device)\n",
        "model_ablation = E8CodeFusion(use_triality=False).to(device)\n",
        "\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "scaler = torch.amp.GradScaler('cuda') if use_amp else nullcontext()\n",
        "\n",
        "opt_ablation = torch.optim.AdamW(model_ablation.parameters(), lr=lr)\n",
        "scaler_ablation = torch.amp.GradScaler('cuda') if use_amp else nullcontext()\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "loss_hist = []\n",
        "loss_abl_hist = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Triality training (full accum)\n",
        "    opt.zero_grad(set_to_none=True)\n",
        "    accum_loss = 0\n",
        "    for accum_step in range(accum_steps):\n",
        "        should_retain_graph = (accum_step < accum_steps - 1)\n",
        "\n",
        "        with torch.amp.autocast(device_type='cuda', dtype=torch.float16) if use_amp else nullcontext():\n",
        "            logits = model(real_data, epoch)\n",
        "            loss = loss_fn(logits.view(-1, tokenizer.vocab_size), target.view(-1)) / accum_steps\n",
        "        scaler.scale(loss).backward(retain_graph=should_retain_graph) if use_amp else loss.backward(retain_graph=should_retain_graph)\n",
        "        accum_loss += loss.item() * accum_steps\n",
        "    scaler.unscale_(opt) if use_amp else None\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1e6)\n",
        "    scaler.step(opt) if use_amp else opt.step()\n",
        "    scaler.update() if use_amp else None\n",
        "    loss_hist.append(accum_loss)\n",
        "\n",
        "    # Ablation training (full accum, separate)\n",
        "    opt_ablation.zero_grad(set_to_none=True)\n",
        "    accum_loss_abl = 0\n",
        "    for accum_step in range(accum_steps):\n",
        "        should_retain_graph = (accum_step < accum_steps - 1)\n",
        "\n",
        "        with torch.amp.autocast(device_type='cuda', dtype=torch.float16) if use_amp else nullcontext():\n",
        "            logits_abl = model_ablation(real_data, epoch)\n",
        "            loss_abl = loss_fn(logits_abl.view(-1, tokenizer.vocab_size), target.view(-1)) / accum_steps\n",
        "        scaler_ablation.scale(loss_abl).backward(retain_graph=should_retain_graph) if use_amp else loss_abl.backward(retain_graph=should_retain_graph)\n",
        "        accum_loss_abl += loss_abl.item() * accum_steps\n",
        "\n",
        "    scaler_ablation.unscale_(opt_ablation) if use_amp else None\n",
        "    torch.nn.utils.clip_grad_norm_(model_ablation.parameters(), 1e6)\n",
        "    scaler_ablation.step(opt_ablation) if use_amp else opt_ablation.step()\n",
        "    scaler_ablation.update() if use_amp else None\n",
        "    loss_abl_hist.append(accum_loss_abl)\n",
        "\n",
        "    if epoch % 500 == 0:\n",
        "        print(f\"Epoch {epoch} | Triality Loss {accum_loss:.6f} | Ablation Loss {accum_loss_abl:.6f}\")\n",
        "\n",
        "# Final Sigma Test\n",
        "triality_mean = np.mean(loss_hist)\n",
        "abl_mean = np.mean(loss_abl_hist)\n",
        "std = np.std(loss_hist + loss_abl_hist)\n",
        "sigma = (abl_mean - triality_mean) / std if std > 0 else 0\n",
        "\n",
        "print(f\"Final Sigma (Triality vs Ablation): {sigma:.2f} (higher = triality advantage)\")\n",
        "\n",
        "print(\"Sim complete — epochs + sigma test done\")"
      ]
    }
  ]
}