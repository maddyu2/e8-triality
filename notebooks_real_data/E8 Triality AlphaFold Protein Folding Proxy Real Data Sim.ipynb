{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c137d4fc-aa70-495f-9f71-0e296e527b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.4.1+cu124)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.19.1+cu124)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.8-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (52 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.2.65 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.2.65)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.0.44 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.0.44)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.119 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.119)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.0.99 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.0.99)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.0.142 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.0.142)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.99)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.99)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (10.2.0)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.61.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (114 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.9-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.1)\n",
      "Collecting pyparsing>=3 (from matplotlib)\n",
      "  Downloading pyparsing-3.3.2-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch) (1.3.0)\n",
      "Downloading matplotlib-3.10.8-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (355 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.61.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.9-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.3.2-py3-none-any.whl (122 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "  Attempting uninstall: pyparsing\n",
      "    Found existing installation: pyparsing 2.4.7\n",
      "    Uninstalling pyparsing-2.4.7:\n",
      "      Successfully uninstalled pyparsing-2.4.7\n",
      "Successfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.61.1 kiwisolver-1.4.9 matplotlib-3.10.8 pyparsing-3.3.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Using device: cuda\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.7%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "\n",
      "=== Training on Task 0 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0 Epoch 0 | Triality Loss 0.185709 | Ablation Loss 0.254059\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26ceaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First cell: Keep-alive + installs (run once — prevents disconnects)\n",
    "from IPython.display import display, Javascript\n",
    "display(Javascript('''\n",
    "function ClickConnect(){\n",
    "  console.log(\"Keeping alive\"); \n",
    "  document.querySelector(\"colab-connect-button\")?.click()\n",
    "}\n",
    "setInterval(ClickConnect,60000)\n",
    "'''))\n",
    "print(\"Keep-alive activated — no disconnect curse\")\n",
    "\n",
    "!pip install torch matplotlib numpy\n",
    "\n",
    "# Second cell: The sim code (fixed + optimized — 3000 epochs, progress prints + visualization)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.amp\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import numpy as np\n",
    "from contextlib import nullcontext\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time  # for timing\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# CONFIG – optimized for speed + stability\n",
    "triality = 3\n",
    "dim = 384\n",
    "latent_dim = 8\n",
    "seq_len = 512  # reduced\n",
    "batch_size = 32  # reduced\n",
    "epochs = 3000  # reduced for fast run (sigma trend visible early)\n",
    "lr = 5e-5\n",
    "use_amp = True\n",
    "use_checkpoint = True\n",
    "depth = 16  # reduced\n",
    "\n",
    "checkpoint_dir = \"./checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "checkpoint_path = os.path.join(checkpoint_dir, \"alphafold_folding_checkpoint.pth\")\n",
    "\n",
    "# Synthetic AlphaFold protein folding proxy (amino acid sequences + mutations/noise/occlusion)\n",
    "features_per_residue = 128\n",
    "\n",
    "protein_data = []\n",
    "for b in range(batch_size):\n",
    "    t = torch.linspace(0, 10*math.pi, seq_len, device=device)\n",
    "    \n",
    "    # Coherent folding trajectory (structured patterns)\n",
    "    folding = torch.sin(t.unsqueeze(-1) * torch.arange(features_per_residue, device=device)) * 0.5\n",
    "    \n",
    "    # Mutations/perturbations (folding chaos proxy)\n",
    "    folding += torch.randn_like(folding) * 0.15\n",
    "    \n",
    "    protein_data.append(folding)\n",
    "\n",
    "protein_data = torch.stack(protein_data).to(device)\n",
    "\n",
    "# Project to shared dim\n",
    "proj = nn.Linear(features_per_residue, dim).to(device)\n",
    "clean_data = proj(protein_data)\n",
    "\n",
    "# High masking (70–90% — residue dropout/mutation proxy)\n",
    "missing_rate = torch.linspace(0.7, 0.9, batch_size, device=device).view(batch_size, 1, 1)\n",
    "mask = torch.rand_like(clean_data) < missing_rate\n",
    "real_data = clean_data.clone()\n",
    "real_data[mask] = 0\n",
    "\n",
    "target = clean_data.detach()  # detach to avoid graph issues\n",
    "\n",
    "# E8 roots – precompute\n",
    "def get_e8_roots():\n",
    "    roots = []\n",
    "    for i in range(8):\n",
    "        for j in range(i+1, 8):\n",
    "            for signs in [(1,1), (1,-1), (-1,1), (-1,-1)]:\n",
    "                v = torch.zeros(8)\n",
    "                v[i] = signs[0]; v[j] = signs[1]\n",
    "                roots.append(v); roots.append(-v)\n",
    "    for signs in range(1 << 8):\n",
    "        v = torch.tensor([(1 if (signs & (1<<k)) else -1) for k in range(8)], dtype=torch.float32) * 0.5\n",
    "        if bin(signs).count('1') % 2 == 0:\n",
    "            roots.append(v); roots.append(-v)\n",
    "    roots = torch.stack(roots[:240])\n",
    "    return roots / roots.norm(dim=-1, keepdim=True)\n",
    "\n",
    "e8_roots = get_e8_roots().to(device)\n",
    "\n",
    "# Triality Cycle Block (detached pump scalar)\n",
    "class FoldingCycleBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(latent_dim, dim // triality, bias=False)\n",
    "        self.register_buffer('roots', e8_roots)\n",
    "\n",
    "    def forward(self, x, step):\n",
    "        pos_emb = self.roots[torch.arange(x.shape[1], device=device) % 240]\n",
    "        low_dim = self.proj(pos_emb)\n",
    "        emb = low_dim.repeat(1, triality)\n",
    "        with torch.no_grad():\n",
    "            pump_scalar = 0.8 * math.sin(step * 0.006 * 2 * math.pi)\n",
    "        pump = torch.full((1, x.shape[1], 1), pump_scalar, device=device)\n",
    "        emb_broadcast = emb.unsqueeze(0)\n",
    "        x_rot1 = x * (emb_broadcast.cos() + pump)\n",
    "        x_rot2 = torch.roll(x_rot1, shifts=1, dims=1) * emb_broadcast.sin()\n",
    "        x_rot3 = torch.roll(x_rot2, shifts=1, dims=1) * emb_broadcast.cos()\n",
    "        fused = (x_rot1 + x_rot2 + x_rot3) / triality\n",
    "        return fused\n",
    "\n",
    "# Dummy cycle for ablation\n",
    "class DummyCycle(nn.Module):\n",
    "    def forward(self, x, step=None):\n",
    "        return x\n",
    "\n",
    "# Model with ablation support\n",
    "class E8FoldingFusion(nn.Module):\n",
    "    def __init__(self, depth=depth, use_triality=True):\n",
    "        super().__init__()\n",
    "        self.use_triality = use_triality\n",
    "        self.cycle = FoldingCycleBlock() if use_triality else DummyCycle()\n",
    "        self.layers = nn.ModuleList([nn.MultiheadAttention(dim, triality if use_triality else 8, batch_first=True) for _ in range(depth)])\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.head = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x, step):\n",
    "        x = self.cycle(x, step)\n",
    "        for layer in self.layers:\n",
    "            if use_checkpoint:\n",
    "                attn, _ = checkpoint(layer, x, x, x, use_reentrant=False)\n",
    "            else:\n",
    "                attn, _ = layer(x, x, x)\n",
    "            x = x + attn\n",
    "            x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "# Models\n",
    "model = E8FoldingFusion(use_triality=True).to(device)\n",
    "model_ablation = E8FoldingFusion(use_triality=False).to(device)\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scaler = torch.amp.GradScaler('cuda') if use_amp else nullcontext()\n",
    "\n",
    "opt_ablation = torch.optim.AdamW(model_ablation.parameters(), lr=lr)\n",
    "scaler_ablation = torch.amp.GradScaler('cuda') if use_amp else nullcontext()\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "loss_hist = []\n",
    "loss_abl_hist = []\n",
    "\n",
    "start_epoch = 0\n",
    "start_time = time.time()\n",
    "\n",
    "# Load checkpoint if exists (resume on disconnect)\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    model_ablation.load_state_dict(checkpoint['model_ablation'])\n",
    "    opt.load_state_dict(checkpoint['opt'])\n",
    "    opt_ablation.load_state_dict(checkpoint['opt_ablation'])\n",
    "    scaler.load_state_dict(checkpoint['scaler'])\n",
    "    scaler_ablation.load_state_dict(checkpoint['scaler_ablation'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    loss_hist = checkpoint['loss_hist']\n",
    "    loss_abl_hist = checkpoint['loss_abl_hist']\n",
    "    print(f\"Resumed from epoch {start_epoch}\")\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    epoch_start = time.time()\n",
    "    # Triality independent training\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    with torch.amp.autocast(device_type='cuda', dtype=torch.float16) if use_amp else nullcontext():\n",
    "        recon = model(real_data, epoch)\n",
    "        loss = loss_fn(recon, target)\n",
    "    scaler.scale(loss).backward() if use_amp else loss.backward()\n",
    "    scaler.unscale_(opt) if use_amp else None\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1e6)\n",
    "    scaler.step(opt) if use_amp else opt.step()\n",
    "    scaler.update() if use_amp else None\n",
    "    loss_hist.append(loss.item())\n",
    "\n",
    "    # Ablation independent training (new graph)\n",
    "    opt_ablation.zero_grad(set_to_none=True)\n",
    "    with torch.amp.autocast(device_type='cuda', dtype=torch.float16) if use_amp else nullcontext():\n",
    "        recon_abl = model_ablation(real_data, epoch)\n",
    "        loss_abl = loss_fn(recon_abl, target)\n",
    "    scaler_ablation.scale(loss_abl).backward() if use_amp else loss_abl.backward()\n",
    "    scaler_ablation.unscale_(opt_ablation) if use_amp else None\n",
    "    torch.nn.utils.clip_grad_norm_(model_ablation.parameters(), 1e6)\n",
    "    scaler_ablation.step(opt_ablation) if use_amp else opt_ablation.step()\n",
    "    scaler_ablation.update() if use_amp else None\n",
    "    loss_abl_hist.append(loss_abl.item())\n",
    "\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    remaining_epochs = epochs - (epoch + 1)\n",
    "    estimated_remaining = remaining_epochs * epoch_time / 3600  # hours\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch {epoch} | Triality Loss {loss.item():.6f} | Ablation Loss {loss_abl.item():.6f} | Time per epoch: {epoch_time:.2f}s | Estimated remaining: {estimated_remaining:.2f} hours\")\n",
    "\n",
    "    # Checkpoint every 1000 epochs\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model': model.state_dict(),\n",
    "            'model_ablation': model_ablation.state_dict(),\n",
    "            'opt': opt.state_dict(),\n",
    "            'opt_ablation': opt_ablation.state_dict(),\n",
    "            'scaler': scaler.state_dict(),\n",
    "            'scaler_ablation': scaler_ablation.state_dict(),\n",
    "            'loss_hist': loss_hist,\n",
    "            'loss_abl_hist': loss_abl_hist,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved at epoch {epoch}\")\n",
    "\n",
    "# Final Sigma Test\n",
    "triality_mean = np.mean(loss_hist)\n",
    "abl_mean = np.mean(loss_abl_hist)\n",
    "std = np.std(loss_hist + loss_abl_hist)\n",
    "sigma = (abl_mean - triality_mean) / std if std > 0 else 0\n",
    "\n",
    "print(f\"Final Sigma (Triality vs Ablation): {sigma:.2f} (higher = triality advantage)\")\n",
    "\n",
    "# Visualization: Folding Trajectory Reconstruction (first residue feature channel proxy)\n",
    "model.eval()\n",
    "model_ablation.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Fresh test batch for viz\n",
    "    test_data = []\n",
    "    for b in range(8):\n",
    "        t = torch.linspace(0, 10*math.pi, seq_len, device=device)\n",
    "        folding = torch.sin(t.unsqueeze(-1) * torch.arange(features_per_residue, device=device)) * 0.5\n",
    "        folding += torch.randn_like(folding) * 0.15\n",
    "        test_data.append(folding)\n",
    "    test_data = torch.stack(test_data).to(device)\n",
    "\n",
    "    clean = proj(test_data)\n",
    "\n",
    "    mask = torch.rand_like(clean) < 0.8\n",
    "    masked = clean.clone()\n",
    "    masked[mask] = 0\n",
    "\n",
    "    recon = model(masked, 0)\n",
    "    recon_abl = model_ablation(masked, 0)\n",
    "\n",
    "    # Plot first residue feature channel (folding proxy)\n",
    "    orig = clean.cpu().numpy()[:, :, 0]\n",
    "    masked_plot = masked.cpu().numpy()[:, :, 0]\n",
    "    tri = recon.cpu().numpy()[:, :, 0]\n",
    "    abl = recon_abl.cpu().numpy()[:, :, 0]\n",
    "\n",
    "    fig, axes = plt.subplots(4, 8, figsize=(16, 8))\n",
    "    for i in range(8):\n",
    "        axes[0, i].plot(orig[i])\n",
    "        axes[0, i].set_title(\"Original Folding\")\n",
    "        axes[1, i].plot(masked_plot[i])\n",
    "        axes[1, i].set_title(\"Masked (80%)\")\n",
    "        axes[2, i].plot(tri[i])\n",
    "        axes[2, i].set_title(\"Triality Recon\")\n",
    "        axes[3, i].plot(abl[i])\n",
    "        axes[3, i].set_title(\"Ablation Recon\")\n",
    "    plt.suptitle(\"E8 Triality AlphaFold Protein Folding Visualization\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Sim complete — visualization displayed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003b0d9f-3156-43a7-aeee-f5d5ef98c1e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
