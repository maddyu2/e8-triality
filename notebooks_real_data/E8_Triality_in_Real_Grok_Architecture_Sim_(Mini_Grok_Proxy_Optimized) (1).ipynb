{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "H100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4N545WkOepE",
        "outputId": "ee9ddf95-6649-45b2-f8c9-399c32178cec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Epoch 0 | Triality Loss 0.850489 | Ablation Loss 0.836162\n",
            "Epoch 500 | Triality Loss 0.688533 | Ablation Loss 0.676749\n",
            "Checkpoint saved at epoch 999\n",
            "Epoch 1000 | Triality Loss 0.634558 | Ablation Loss 0.625433\n",
            "Epoch 1500 | Triality Loss 0.589428 | Ablation Loss 0.581777\n",
            "Checkpoint saved at epoch 1999\n",
            "Epoch 2000 | Triality Loss 0.545486 | Ablation Loss 0.539314\n",
            "Epoch 2500 | Triality Loss 0.502906 | Ablation Loss 0.497996\n",
            "Checkpoint saved at epoch 2999\n",
            "Final Sigma (Triality vs Ablation): -0.09 (higher = triality advantage)\n",
            "Sim complete — epochs + sigma test done\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.amp\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "import numpy as np\n",
        "from contextlib import nullcontext\n",
        "import math\n",
        "import os\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# CONFIG – optimized for T4 stability (reduced for no OOM)\n",
        "triality = 3\n",
        "dim = 240  # Changed from 256 to 240 to be divisible by triality (3) and num_heads (8)\n",
        "latent_dim = 8\n",
        "seq_len = 512  # reduced\n",
        "batch_size = 32  # reduced\n",
        "epochs = 3000  # reduced for fast run (sigma trend early)\n",
        "lr = 5e-5\n",
        "use_amp = True\n",
        "use_checkpoint = True\n",
        "\n",
        "checkpoint_dir = \"./checkpoints\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "checkpoint_path = os.path.join(checkpoint_dir, \"multimodal_noise_checkpoint.pth\")\n",
        "\n",
        "# Synthetic Grok multimodal noise adaptation proxy (video + audio + text features + extreme noise)\n",
        "features_video = 96\n",
        "features_audio = 48\n",
        "features_text = 48\n",
        "\n",
        "multimodal_data = []\n",
        "for b in range(batch_size):\n",
        "    t = torch.linspace(0, 10*math.pi, seq_len, device=device)\n",
        "\n",
        "    video = torch.sin(t.unsqueeze(-1) * torch.arange(features_video, device=device)) * 0.5\n",
        "    audio = torch.cos(t.unsqueeze(-1) * torch.arange(features_audio, device=device) * 1.2) * 0.4\n",
        "    text = torch.sin(t.unsqueeze(-1) * torch.arange(features_text, device=device) * 1.5) * 0.3\n",
        "\n",
        "    frame = torch.cat([video, audio, text], dim=-1)\n",
        "    # Extreme noise for adaptation test\n",
        "    noise_amp = torch.linspace(0.5, 1.0, batch_size, device=device)[b]\n",
        "    frame += torch.randn_like(frame) * noise_amp\n",
        "    multimodal_data.append(frame)\n",
        "\n",
        "multimodal_data = torch.stack(multimodal_data).to(device)\n",
        "\n",
        "# Project to shared dim - proj layer itself should be defined once\n",
        "proj = nn.Linear(features_video + features_audio + features_text, dim).to(device)\n",
        "\n",
        "# target is derived from clean_data, so it should also be created once\n",
        "# Detach target from the graph as it's ground truth and shouldn't receive gradients\n",
        "target = proj(multimodal_data).detach()\n",
        "\n",
        "# E8 roots – precompute\n",
        "def get_e8_roots():\n",
        "    roots = []\n",
        "    for i in range(8):\n",
        "        for j in range(i+1, 8):\n",
        "            for signs in [(1,1), (1,-1), (-1,1), (-1,-1)]:\n",
        "                v = torch.zeros(8)\n",
        "                v[i] = signs[0]; v[j] = signs[1]\n",
        "                roots.append(v); roots.append(-v)\n",
        "    for signs in range(1 << 8):\n",
        "        v = torch.tensor([(1 if (signs & (1<<k)) else -1) for k in range(8)], dtype=torch.float32) * 0.5\n",
        "        if bin(signs).count('1') % 2 == 0:\n",
        "            roots.append(v); roots.append(-v)\n",
        "    roots = torch.stack(roots[:240])\n",
        "    return roots / roots.norm(dim=-1, keepdim=True)\n",
        "\n",
        "e8_roots = get_e8_roots().to(device)\n",
        "\n",
        "# Triality Cycle Block\n",
        "class NoiseAdaptCycleBlock(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(latent_dim, dim // triality, bias=False)\n",
        "        self.register_buffer('roots', e8_roots)\n",
        "\n",
        "    def forward(self, x, step):\n",
        "        pos_emb = self.roots[torch.arange(x.shape[1], device=device) % 240]\n",
        "        low_dim = self.proj(pos_emb)\n",
        "        emb = low_dim.repeat(1, triality)\n",
        "        with torch.no_grad():\n",
        "            pump_scalar = 0.8 * math.sin(step * 0.006 * 2 * math.pi)\n",
        "        pump = torch.full((1, x.shape[1], 1), pump_scalar, device=device)\n",
        "        emb_broadcast = emb.unsqueeze(0)\n",
        "        x_rot1 = x * (emb_broadcast.cos() + pump)\n",
        "        x_rot2 = torch.roll(x_rot1, shifts=1, dims=1) * emb_broadcast.sin()\n",
        "        x_rot3 = torch.roll(x_rot2, shifts=1, dims=1) * emb_broadcast.cos()\n",
        "        fused = (x_rot1 + x_rot2 + x_rot3) / triality\n",
        "        return fused\n",
        "\n",
        "# Dummy cycle for ablation\n",
        "class DummyCycle(nn.Module):\n",
        "    def forward(self, x, step=None):\n",
        "        return x\n",
        "\n",
        "# Model with ablation support (reduced depth)\n",
        "class E8NoiseAdaptFusion(nn.Module):\n",
        "    def __init__(self, depth=16, use_triality=True):  # reduced\n",
        "        super().__init__()\n",
        "        self.use_triality = use_triality\n",
        "        self.cycle = NoiseAdaptCycleBlock() if use_triality else DummyCycle()\n",
        "        # Fix: ensure num_heads (8) divides embed_dim (dim=256)\n",
        "        self.layers = nn.ModuleList([nn.MultiheadAttention(dim, 8, batch_first=True) for _ in range(depth)])\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.head = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x, step):\n",
        "        x = self.cycle(x, step)\n",
        "        for layer in self.layers:\n",
        "            if use_checkpoint:\n",
        "                attn, _ = checkpoint(layer, x, x, x, use_reentrant=False)\n",
        "            else:\n",
        "                attn, _ = layer(x, x, x)\n",
        "            x = x + attn\n",
        "            x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "# Models\n",
        "model = E8NoiseAdaptFusion(use_triality=True).to(device)\n",
        "model_ablation = E8NoiseAdaptFusion(use_triality=False).to(device)\n",
        "\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "scaler = torch.amp.GradScaler('cuda') if use_amp else nullcontext()\n",
        "\n",
        "opt_ablation = torch.optim.AdamW(model_ablation.parameters(), lr=lr)\n",
        "scaler_ablation = torch.amp.GradScaler('cuda') if use_amp else nullcontext()\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "loss_hist = []\n",
        "loss_abl_hist = []\n",
        "\n",
        "start_epoch = 0\n",
        "\n",
        "# Load checkpoint if exists (resume on disconnect)\n",
        "if os.path.exists(checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['model'])\n",
        "    model_ablation.load_state_dict(checkpoint['model_ablation'])\n",
        "    opt.load_state_dict(checkpoint['opt'])\n",
        "    opt_ablation.load_state_dict(checkpoint['opt_ablation'])\n",
        "    scaler.load_state_dict(checkpoint['scaler'])\n",
        "    scaler_ablation.load_state_dict(checkpoint['scaler_ablation'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    loss_hist = checkpoint['loss_hist']\n",
        "    loss_abl_hist = checkpoint['loss_abl_hist']\n",
        "    print(f\"Resumed from epoch {start_epoch}\")\n",
        "\n",
        "for epoch in range(start_epoch, epochs):\n",
        "    opt.zero_grad(set_to_none=True)\n",
        "    opt_ablation.zero_grad(set_to_none=True)\n",
        "\n",
        "    # High masking (70–90% — partial input + noise adaptation proxy)\n",
        "    # Recompute clean_data, mask, and real_data each epoch to ensure fresh computational graph\n",
        "    # Detach target from proj's graph, then re-enable grad tracking for real_data_current_epoch\n",
        "    clean_data_current_epoch = proj(multimodal_data).detach() # Detach from proj's graph\n",
        "    missing_rate_current_epoch = torch.linspace(0.7, 0.9, batch_size, device=device).view(batch_size, 1, 1)\n",
        "    mask_current_epoch = torch.rand_like(clean_data_current_epoch) < missing_rate_current_epoch\n",
        "\n",
        "    # Apply masking using masked_fill to create a new tensor, then make it require gradients\n",
        "    real_data_current_epoch = clean_data_current_epoch.masked_fill(mask_current_epoch, 0.0).requires_grad_(True)\n",
        "\n",
        "    with torch.amp.autocast(device_type='cuda', dtype=torch.float16) if use_amp else nullcontext():\n",
        "        # Pass the newly created real_data for the current epoch\n",
        "        recon = model(real_data_current_epoch, epoch)\n",
        "        loss = loss_fn(recon, target)\n",
        "\n",
        "        recon_abl = model_ablation(real_data_current_epoch, epoch)\n",
        "        loss_abl = loss_fn(recon_abl, target)\n",
        "\n",
        "    # With detached inputs from proj, the graphs for model and model_ablation are independent.\n",
        "    # So, retain_graph=True is not necessary for the first backward pass.\n",
        "    scaler.scale(loss).backward() if use_amp else loss.backward()\n",
        "    scaler.unscale_(opt) if use_amp else None\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1e6)\n",
        "    scaler.step(opt) if use_amp else opt.step()\n",
        "    scaler.update() if use_amp else None\n",
        "\n",
        "    scaler_ablation.scale(loss_abl).backward() if use_amp else loss_abl.backward()\n",
        "    scaler_ablation.unscale_(opt_ablation) if use_amp else None\n",
        "    torch.nn.utils.clip_grad_norm_(model_ablation.parameters(), 1e6)\n",
        "    scaler_ablation.step(opt_ablation) if use_amp else opt_ablation.step()\n",
        "    scaler_ablation.update() if use_amp else None\n",
        "\n",
        "    loss_hist.append(loss.item())\n",
        "    loss_abl_hist.append(loss_abl.item())\n",
        "\n",
        "    if epoch % 500 == 0:\n",
        "        print(f\"Epoch {epoch} | Triality Loss {loss.item():.6f} | Ablation Loss {loss_abl.item():.6f}\")\n",
        "\n",
        "    # Checkpoint every 1000 epochs\n",
        "    if (epoch + 1) % 1000 == 0:\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model': model.state_dict(),\n",
        "            'model_ablation': model_ablation.state_dict(),\n",
        "            'opt': opt.state_dict(),\n",
        "            'opt_ablation': opt_ablation.state_dict(),\n",
        "            'scaler': scaler.state_dict(),\n",
        "            'scaler_ablation': scaler_ablation.state_dict(),\n",
        "            'loss_hist': loss_hist,\n",
        "            'loss_abl_hist': loss_abl_hist,\n",
        "        }, checkpoint_path)\n",
        "        print(f\"Checkpoint saved at epoch {epoch}\")\n",
        "\n",
        "# Final Sigma Test\n",
        "triality_mean = np.mean(loss_hist)\n",
        "abl_mean = np.mean(loss_abl_hist)\n",
        "std = np.std(loss_hist + loss_abl_hist)\n",
        "sigma = (abl_mean - triality_mean) / std if std > 0 else 0\n",
        "\n",
        "print(f\"Final Sigma (Triality vs Ablation): {sigma:.2f} (higher = triality advantage)\")\n",
        "\n",
        "print(\"Sim complete — epochs + sigma test done\")"
      ]
    }
  ]
}