{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# First cell: Install dependencies (run once — fast on RunPod H100)\n",
        "!pip install torch matplotlib numpy\n",
        "\n",
        "# Second cell: The sim code (fixed — no backward error)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.amp\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "import numpy as np\n",
        "from contextlib import nullcontext\n",
        "import math\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# CONFIG – optimized for H100 speed (fast epochs)\n",
        "triality = 3\n",
        "dim = 384\n",
        "latent_dim = 8\n",
        "seq_len = 1024\n",
        "batch_size = 32\n",
        "epochs = 20000\n",
        "lr = 5e-5\n",
        "use_amp = True\n",
        "use_checkpoint = True\n",
        "\n",
        "# Synthetic CRISPR gRNA proxy\n",
        "alphabet = torch.tensor([0, 1, 2, 3], device=device)\n",
        "gRNAs = []\n",
        "for b in range(batch_size):\n",
        "    base = torch.sin(torch.linspace(0, 10*math.pi, seq_len, device=device)) * 1.5 + 2\n",
        "    base = base % 4\n",
        "    gRNA = base.long() + torch.randint(-1, 2, (seq_len,), device=device)\n",
        "    gRNA = torch.clamp(gRNA, 0, 3)\n",
        "    gRNAs.append(gRNA)\n",
        "\n",
        "gRNAs = torch.stack(gRNAs).to(device)\n",
        "\n",
        "embed = nn.Embedding(4, dim).to(device)\n",
        "clean_data = embed(gRNAs)\n",
        "\n",
        "missing_rate = torch.linspace(0.4, 0.7, batch_size, device=device).view(batch_size, 1, 1)\n",
        "mask = torch.rand_like(clean_data) < missing_rate\n",
        "real_data = clean_data.clone()\n",
        "real_data[mask] = 0\n",
        "real_data = real_data.detach() # Ensure real_data is a leaf tensor for the training loop\n",
        "\n",
        "target = gRNAs\n",
        "\n",
        "# E8 roots – precompute\n",
        "def get_e8_roots():\n",
        "    roots = []\n",
        "    for i in range(8):\n",
        "        for j in range(i+1, 8):\n",
        "            for signs in [(1,1), (1,-1), (-1,1), (-1,-1)]:\n",
        "                v = torch.zeros(8)\n",
        "                v[i] = signs[0]; v[j] = signs[1]\n",
        "                roots.append(v); roots.append(-v)\n",
        "    for signs in range(1 << 8):\n",
        "        v = torch.tensor([(1 if (signs & (1<<k)) else -1) for k in range(8)], dtype=torch.float32) * 0.5\n",
        "        if bin(signs).count('1') % 2 == 0:\n",
        "            roots.append(v); roots.append(-v)\n",
        "    roots = torch.stack(roots[:240])\n",
        "    return roots / roots.norm(dim=-1, keepdim=True)\n",
        "\n",
        "e8_roots = get_e8_roots().to(device)\n",
        "\n",
        "# Triality Cycle Block (detached pump scalar)\n",
        "class CRISPRCycleBlock(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(latent_dim, dim // triality, bias=False)\n",
        "        self.register_buffer('roots', e8_roots)\n",
        "\n",
        "    def forward(self, x, step):\n",
        "        pos_emb = self.roots[torch.arange(x.shape[1], device=device) % 240]\n",
        "        low_dim = self.proj(pos_emb)\n",
        "        emb = low_dim.repeat(1, triality)\n",
        "        # Detached pump scalar\n",
        "        with torch.no_grad():\n",
        "            pump_scalar = 0.8 * math.sin(step * 0.006 * 2 * math.pi)\n",
        "        pump = torch.full((1, x.shape[1], 1), pump_scalar, device=device)\n",
        "        emb_broadcast = emb.unsqueeze(0)\n",
        "        x_rot1 = x * (emb_broadcast.cos() + pump)\n",
        "        x_rot2 = torch.roll(x_rot1, shifts=1, dims=1) * emb_broadcast.sin()\n",
        "        x_rot3 = torch.roll(x_rot2, shifts=1, dims=1) * emb_broadcast.cos()\n",
        "        fused = (x_rot1 + x_rot2 + x_rot3) / triality\n",
        "        return fused\n",
        "\n",
        "# Dummy cycle for ablation\n",
        "class DummyCycle(nn.Module):\n",
        "    def forward(self, x, step=None):\n",
        "        return x\n",
        "\n",
        "# Model with ablation support\n",
        "class E8CRISPRFusion(nn.Module):\n",
        "    def __init__(self, depth=32, use_triality=True):\n",
        "        super().__init__()\n",
        "        self.use_triality = use_triality\n",
        "        self.cycle = CRISPRCycleBlock() if use_triality else DummyCycle()\n",
        "        self.layers = nn.ModuleList([nn.MultiheadAttention(dim, triality if use_triality else 8, batch_first=True) for _ in range(depth)])\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.head = nn.Linear(dim, 4)\n",
        "\n",
        "    def forward(self, x, step):\n",
        "        x = self.cycle(x, step)\n",
        "        for layer in self.layers:\n",
        "            if use_checkpoint:\n",
        "                attn, _ = checkpoint(layer, x, x, x, use_reentrant=False)\n",
        "            else:\n",
        "                attn, _ = layer(x, x, x)\n",
        "            x = x + self.norm(attn)\n",
        "        return self.head(x)\n",
        "\n",
        "# Models\n",
        "model = E8CRISPRFusion(use_triality=True).to(device)\n",
        "model_ablation = E8CRISPRFusion(use_triality=False).to(device)\n",
        "\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "scaler = torch.amp.GradScaler('cuda') if use_amp else nullcontext()\n",
        "\n",
        "opt_ablation = torch.optim.AdamW(model_ablation.parameters(), lr=lr)\n",
        "scaler_ablation = torch.amp.GradScaler('cuda') if use_amp else nullcontext()\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "loss_hist = []\n",
        "loss_abl_hist = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # --- Training for model (with triality) ---\n",
        "    opt.zero_grad(set_to_none=True)\n",
        "    with torch.amp.autocast(device_type='cuda', dtype=torch.float16) if use_amp else nullcontext():\n",
        "        logits = model(real_data, epoch)\n",
        "        loss = loss_fn(logits.view(-1, 4), target.view(-1))\n",
        "\n",
        "    scaler.scale(loss).backward(retain_graph=True) if use_amp else loss.backward(retain_graph=True)\n",
        "    scaler.unscale_(opt) if use_amp else None\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1e6)\n",
        "    scaler.step(opt) if use_amp else opt.step()\n",
        "    scaler.update() if use_amp else None\n",
        "\n",
        "    # --- Training for model_ablation (without triality) ---\n",
        "    opt_ablation.zero_grad(set_to_none=True)\n",
        "    with torch.amp.autocast(device_type='cuda', dtype=torch.float16) if use_amp else nullcontext():\n",
        "        logits_abl = model_ablation(real_data, epoch) # Pass real_data again to create a fresh graph\n",
        "        loss_abl = loss_fn(logits_abl.view(-1, 4), target.view(-1))\n",
        "\n",
        "    scaler_ablation.scale(loss_abl).backward() if use_amp else loss_abl.backward()\n",
        "    scaler_ablation.unscale_(opt_ablation) if use_amp else None\n",
        "    torch.nn.utils.clip_grad_norm_(model_ablation.parameters(), 1e6)\n",
        "    scaler_ablation.step(opt_ablation) if use_amp else opt_ablation.step()\n",
        "    scaler_ablation.update() if use_amp else None\n",
        "\n",
        "    loss_hist.append(loss.item())\n",
        "    loss_abl_hist.append(loss_abl.item())\n",
        "\n",
        "    if epoch % 500 == 0:\n",
        "        print(f\"Epoch {epoch} | Triality Loss {loss.item():.6f} | Ablation Loss {loss_abl.item():.6f}\")\n",
        "\n",
        "# Final Sigma Test\n",
        "triality_mean = np.mean(loss_hist)\n",
        "abl_mean = np.mean(loss_abl_hist)\n",
        "std = np.std(loss_hist + loss_abl_hist)\n",
        "sigma = (abl_mean - triality_mean) / std if std > 0 else 0\n",
        "\n",
        "print(f\"Final Sigma (Triality vs Ablation): {sigma:.2f} (higher = triality advantage)\")\n",
        "\n",
        "print(\"Sim complete — epochs + sigma test done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuDxngTz_CKu",
        "outputId": "d5a7b5f1-1842-4c87-a712-e2419813db27"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Using device: cuda\n",
            "Epoch 0 | Triality Loss 1.739608 | Ablation Loss 3.377460\n",
            "Epoch 500 | Triality Loss 0.000136 | Ablation Loss 0.000211\n",
            "Epoch 1000 | Triality Loss 0.000044 | Ablation Loss 0.000051\n",
            "Epoch 1500 | Triality Loss 0.000007 | Ablation Loss 0.000025\n",
            "Epoch 2000 | Triality Loss 0.000025 | Ablation Loss 0.000015\n",
            "Epoch 2500 | Triality Loss 0.000102 | Ablation Loss 0.000009\n",
            "Epoch 3000 | Triality Loss 0.000002 | Ablation Loss 0.000006\n",
            "Epoch 3500 | Triality Loss 0.000001 | Ablation Loss 0.000005\n",
            "Epoch 4000 | Triality Loss 0.000001 | Ablation Loss 0.000004\n",
            "Epoch 4500 | Triality Loss 0.000001 | Ablation Loss 0.000002\n",
            "Epoch 5000 | Triality Loss 0.000000 | Ablation Loss 0.000002\n",
            "Epoch 5500 | Triality Loss 0.000000 | Ablation Loss 0.000001\n",
            "Epoch 6000 | Triality Loss 0.000000 | Ablation Loss 0.000001\n",
            "Epoch 6500 | Triality Loss 0.000000 | Ablation Loss 0.000001\n",
            "Epoch 7000 | Triality Loss 0.000000 | Ablation Loss 0.000001\n",
            "Epoch 7500 | Triality Loss 0.000000 | Ablation Loss 0.000001\n",
            "Epoch 8000 | Triality Loss 0.000000 | Ablation Loss 0.000000\n",
            "Epoch 8500 | Triality Loss 0.000000 | Ablation Loss 0.000000\n",
            "Epoch 9000 | Triality Loss 0.000000 | Ablation Loss 0.000000\n",
            "Epoch 9500 | Triality Loss 0.000000 | Ablation Loss 0.000000\n",
            "Epoch 10000 | Triality Loss 0.000000 | Ablation Loss 0.000000\n",
            "Epoch 10500 | Triality Loss 0.000000 | Ablation Loss 0.000000\n",
            "Epoch 11000 | Triality Loss 0.000000 | Ablation Loss 0.000000\n",
            "Epoch 11500 | Triality Loss 0.000000 | Ablation Loss 0.000000\n",
            "Epoch 12000 | Triality Loss 0.000000 | Ablation Loss 0.000000\n",
            "Epoch 12500 | Triality Loss 0.000000 | Ablation Loss 0.000000\n",
            "Epoch 13000 | Triality Loss 0.000000 | Ablation Loss 0.000000\n",
            "Epoch 13500 | Triality Loss 0.000000 | Ablation Loss 0.000000\n",
            "Epoch 14000 | Triality Loss 0.000000 | Ablation Loss 0.000000\n",
            "Epoch 14500 | Triality Loss 0.000000 | Ablation Loss 0.000000\n",
            "Epoch 15000 | Triality Loss 0.000000 | Ablation Loss 0.000000\n",
            "Epoch 15500 | Triality Loss 0.000000 | Ablation Loss 0.000000\n",
            "Epoch 16000 | Triality Loss 0.000000 | Ablation Loss 0.000000\n",
            "Epoch 16500 | Triality Loss 0.000000 | Ablation Loss 0.000000\n",
            "Epoch 17000 | Triality Loss 0.000000 | Ablation Loss 0.000000\n",
            "Epoch 17500 | Triality Loss 0.000000 | Ablation Loss 0.000000\n",
            "Epoch 18000 | Triality Loss 0.000000 | Ablation Loss 0.000000\n",
            "Epoch 18500 | Triality Loss 0.000000 | Ablation Loss 0.000000\n",
            "Epoch 19000 | Triality Loss 0.000000 | Ablation Loss 0.000000\n",
            "Epoch 19500 | Triality Loss 0.000000 | Ablation Loss 0.000000\n",
            "Final Sigma (Triality vs Ablation): -0.04 (higher = triality advantage)\n",
            "Sim complete — epochs + sigma test done\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "H100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}